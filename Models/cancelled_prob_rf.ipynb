{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T04:52:24.374443Z",
     "start_time": "2025-04-20T04:52:24.361147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set data paths\n",
    "flight_data_path = './cleaned_data/'\n",
    "weather_data_path = './cleaned_weather_data/'\n",
    "top_airports_file = './top_100_airports.csv'\n",
    "output_dir = './cancelled_prob_rf_models/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'metrics'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)\n",
    "\n",
    "print(\"Starting flight cancellation prediction models with red-eye flight detection (Random Forest)...\")\n",
    "print(f\"Flight data directory: {flight_data_path}\")\n",
    "print(f\"Weather data directory: {weather_data_path}\")\n",
    "print(f\"Top airports file: {top_airports_file}\")\n",
    "print(f\"Model output directory: {output_dir}\")\n",
    "\n",
    "# Load top 30 airports from the top 100 airports file\n",
    "try:\n",
    "    top_airports = pd.read_csv(top_airports_file, low_memory=False)\n",
    "\n",
    "    top_airports = top_airports.head(30)\n",
    "\n",
    "    top_airport_codes = set(top_airports['ORIGIN_IATA'].str.strip().tolist())\n",
    "\n",
    "    print(f\"Loaded top 30 airports: {', '.join(sorted(top_airport_codes))}\")\n",
    "    print(f\"Busiest airport: {top_airports.iloc[0]['ORIGIN_IATA']} with {top_airports.iloc[0]['Times']} flights\")\n",
    "    print(f\"30th busiest airport: {top_airports.iloc[29]['ORIGIN_IATA']} with {top_airports.iloc[29]['Times']} flights\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading top airports file: {e}\")\n",
    "    top_airport_codes = None\n",
    "    print(\"Will process all airports (top airports file not available)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting flight cancellation prediction models with red-eye flight detection (Random Forest)...\n",
      "Flight data directory: ./cleaned_data/\n",
      "Weather data directory: ./cleaned_weather_data/\n",
      "Top airports file: ./top_100_airports.csv\n",
      "Model output directory: ./cancelled_prob_rf_models/\n",
      "Loaded top 30 airports: ATL, AUS, BNA, BOS, BWI, CLT, DCA, DEN, DFW, DTW, EWR, FLL, IAD, IAH, JFK, LAS, LAX, LGA, MCO, MDW, MIA, MSP, ORD, PHL, PHX, SAN, SEA, SFO, SLC, TPA\n",
      "Busiest airport: ATL with 457121 flights\n",
      "30th busiest airport: TPA with 97235 flights\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T04:52:24.394623Z",
     "start_time": "2025-04-20T04:52:24.381450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_weather_data():\n",
    "    print(\"\\nLoading weather data...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_files = glob.glob(os.path.join(weather_data_path, \"*.csv\"))\n",
    "    print(f\"Found {len(all_files)} total weather data files\")\n",
    "    weather_dict = {}\n",
    "    count = 0\n",
    "    matching_count = 0\n",
    "\n",
    "    # Process all weather files\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            filename = os.path.basename(file)\n",
    "            parts = filename.split('.')[0].split('_')\n",
    "\n",
    "            if len(parts) >= 3:\n",
    "                iata = parts[0]\n",
    "                year = parts[1]\n",
    "                month_name = parts[2]\n",
    "\n",
    "                month_map = {\n",
    "                    'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "                    'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "                    'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "                }\n",
    "\n",
    "                if month_name in month_map:\n",
    "                    month = month_map[month_name]\n",
    "\n",
    "                    if top_airport_codes is None or iata in top_airport_codes:\n",
    "                        weather_data = pd.read_csv(file, low_memory=False)\n",
    "\n",
    "                        if 'DATE' not in weather_data.columns:\n",
    "                            print(f\"Warning: DATE column not found in {filename}\")\n",
    "                            continue\n",
    "\n",
    "                        weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "\n",
    "                        key = f\"{iata}_{year}_{month}\"\n",
    "\n",
    "                        # Store the weather data\n",
    "                        weather_dict[key] = weather_data\n",
    "                        matching_count += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Unknown month format in {filename}\")\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                # Print progress periodically\n",
    "                if count % 100 == 0:\n",
    "                    print(f\"Processed {count} weather files, loaded {matching_count} matching files\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading weather file {file}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {matching_count} weather files out of {count} processed files\")\n",
    "    print(f\"Loading weather data took: {time.time() - start_time:.2f} seconds\")\n",
    "    return weather_dict\n",
    "\n",
    "# Function to match weather data to flights for origin airports (departure weather)\n",
    "def match_weather_data(df):\n",
    "    print(\"\\nMatching origin weather data with flights...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    date_columns_exist = all(col in df.columns for col in ['YEAR', 'MONTH', 'DAY'])\n",
    "    if not date_columns_exist:\n",
    "        print(\"Warning: Missing one or more date columns (YEAR, MONTH, DAY)\")\n",
    "        print(\"Weather data cannot be matched\")\n",
    "        return df\n",
    "\n",
    "    df['FLIGHT_DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']])\n",
    "    df['WEATHER_KEY'] = df['ORIGIN_IATA'] + '_' + df['YEAR'].astype(str) + '_' + df['MONTH'].astype(str).str.zfill(2)\n",
    "    weather_columns = ['EXTREME_WEATHER', 'PRCP', 'WT01', 'WT03', 'WT04', 'WT05', 'WT08', 'WT11']\n",
    "    for col in weather_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "\n",
    "    # Process in batches\n",
    "    matched_count = 0\n",
    "    batch_size = 10000\n",
    "\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                weather_key = row['WEATHER_KEY']\n",
    "                flight_date = row['FLIGHT_DATE']\n",
    "\n",
    "                if weather_key in weather_dict:\n",
    "                    weather_data = weather_dict[weather_key]\n",
    "\n",
    "                    matching_weather = weather_data[weather_data['DATE'] == flight_date]\n",
    "\n",
    "                    if not matching_weather.empty:\n",
    "                        for col in weather_columns:\n",
    "                            if col in matching_weather.columns:\n",
    "                                df.at[idx, col] = matching_weather[col].iloc[0]\n",
    "                        matched_count += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed {end_idx}/{len(df)} rows, matched {matched_count} flights with origin weather data\")\n",
    "\n",
    "    print(f\"Matched origin weather data for {matched_count} flights ({matched_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"Origin weather matching took: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T04:52:24.896911Z",
     "start_time": "2025-04-20T04:52:24.415968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function Match destination weather data\n",
    "def match_destination_weather_data(df):\n",
    "    print(\"\\nMatching destination weather data with flights...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    date_columns_exist = all(col in df.columns for col in ['YEAR', 'MONTH', 'DAY'])\n",
    "    if not date_columns_exist:\n",
    "        print(\"Warning: Missing one or more date columns (YEAR, MONTH, DAY)\")\n",
    "        print(\"Destination weather data cannot be matched\")\n",
    "        return df\n",
    "\n",
    "    if 'FLIGHT_DATE' not in df.columns:\n",
    "        df['FLIGHT_DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']])\n",
    "\n",
    "    df['DEST_WEATHER_KEY'] = df['DEST_IATA'] + '_' + df['YEAR'].astype(str) + '_' + df['MONTH'].astype(str).str.zfill(2)\n",
    "\n",
    "    # Create columns for destination weather features with DEST_ prefix\n",
    "    weather_columns = ['EXTREME_WEATHER', 'PRCP', 'WT01', 'WT03', 'WT04', 'WT05', 'WT08', 'WT11']\n",
    "    for col in weather_columns:\n",
    "        dest_col = f'DEST_{col}'\n",
    "        if dest_col not in df.columns:\n",
    "            df[dest_col] = 0.0\n",
    "\n",
    "    # Process in batches\n",
    "    matched_count = 0\n",
    "    batch_size = 10000\n",
    "\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                dest_weather_key = row['DEST_WEATHER_KEY']\n",
    "                flight_date = row['FLIGHT_DATE']\n",
    "\n",
    "                if dest_weather_key in weather_dict:\n",
    "                    weather_data = weather_dict[dest_weather_key]\n",
    "\n",
    "                    matching_weather = weather_data[weather_data['DATE'] == flight_date]\n",
    "\n",
    "                    if not matching_weather.empty:\n",
    "                        for col in weather_columns:\n",
    "                            if col in matching_weather.columns:\n",
    "                                df.at[idx, f'DEST_{col}'] = matching_weather[col].iloc[0]\n",
    "                        matched_count += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        print(f\"Processed {end_idx}/{len(df)} rows, matched {matched_count} flights with destination weather data\")\n",
    "\n",
    "    print(f\"Matched destination weather data for {matched_count} flights ({matched_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"Destination weather matching took: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Get specific May files from the cleaned_data directory based on the file list you shared\n",
    "def get_may_files():\n",
    "    may_files = [\n",
    "        os.path.join(flight_data_path, \"May2021.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2022.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2023.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2024.csv\")\n",
    "    ]\n",
    "\n",
    "    existing_files = []\n",
    "    for file_path in may_files:\n",
    "        if os.path.exists(file_path):\n",
    "            existing_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} not found\")\n",
    "\n",
    "    return existing_files\n",
    "\n",
    "# Get the May 2021-2024 flight data files\n",
    "flight_files = get_may_files()\n",
    "print(f\"\\nFound {len(flight_files)} May files to process:\")\n",
    "for f in flight_files:\n",
    "    print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "if not flight_files:\n",
    "    print(\"No May 2021-2024 files were found. Please check file paths.\")\n",
    "    exit(1)\n",
    "\n",
    "# Load all weather data once (shared across all models)\n",
    "weather_dict = load_weather_data()\n",
    "\n",
    "# Function to extract year from filename (for logging purposes only)\n",
    "def extract_year_from_filename(filename):\n",
    "    base_name = os.path.basename(filename)\n",
    "    year_str = base_name.replace('May', '').split('.')[0]\n",
    "    return int(year_str)\n",
    "\n",
    "# Function to create red-eye flight indicator\n",
    "def create_redeye_indicator(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['IS_REDEYE'] = 0\n",
    "\n",
    "    dep_time_cols = [col for col in df.columns if 'DEP_TIME' in col.upper()]\n",
    "\n",
    "    arr_time_cols = [col for col in df.columns if 'ARR_TIME' in col.upper()]\n",
    "\n",
    "    if dep_time_cols:\n",
    "        dep_time_col = dep_time_cols[0]\n",
    "\n",
    "        if df[dep_time_col].dtype != 'float64':\n",
    "            try:\n",
    "                df[dep_time_col] = df[dep_time_col].astype(float)\n",
    "            except:\n",
    "\n",
    "                print(f\"Warning: Could not convert {dep_time_col} to float\")\n",
    "\n",
    "        redeye_departure = (df[dep_time_col] >= 0) & (df[dep_time_col] < 600)\n",
    "        df.loc[redeye_departure, 'IS_REDEYE'] = 1\n",
    "\n",
    "    # If we have arrival time\n",
    "    if arr_time_cols:\n",
    "        arr_time_col = arr_time_cols[0]\n",
    "\n",
    "        if df[arr_time_col].dtype != 'float64':\n",
    "            try:\n",
    "                df[arr_time_col] = df[arr_time_col].astype(float)\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert {arr_time_col} to float\")\n",
    "\n",
    "        redeye_arrival = (df[arr_time_col] >= 0) & (df[arr_time_col] < 600)\n",
    "        df.loc[redeye_arrival, 'IS_REDEYE'] = 1\n",
    "\n",
    "    redeye_count = df['IS_REDEYE'].sum()\n",
    "    total_count = len(df)\n",
    "    print(f\"Identified {redeye_count} red-eye flights out of {total_count} total flights ({redeye_count/total_count*100:.2f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to create additional indicator variables\n",
    "def create_indicator_variables(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['IS_WEEKEND'] = 0\n",
    "    df['IS_MORNING_PEAK'] = 0\n",
    "    df['IS_EVENING_PEAK'] = 0\n",
    "\n",
    "    if 'WEEK' in df.columns:\n",
    "        weekend_mask = (df['WEEK'] == 0) | (df['WEEK'] == 6)\n",
    "        df.loc[weekend_mask, 'IS_WEEKEND'] = 1\n",
    "\n",
    "    dep_time_cols = [col for col in df.columns if 'DEP_TIME' in col.upper()]\n",
    "\n",
    "    if dep_time_cols:\n",
    "        dep_time_col = dep_time_cols[0]\n",
    "        if df[dep_time_col].dtype != 'float64':\n",
    "            try:\n",
    "                df[dep_time_col] = df[dep_time_col].astype(float)\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert {dep_time_col} to float\")\n",
    "\n",
    "        # Morning peak hours: 7:00 to 10:00 AM (700-1000)\n",
    "        morning_peak = (df[dep_time_col] >= 700) & (df[dep_time_col] < 1000)\n",
    "        df.loc[morning_peak, 'IS_MORNING_PEAK'] = 1\n",
    "\n",
    "        # Evening peak hours: 4:00 to 7:00 PM (1600-1900)\n",
    "        evening_peak = (df[dep_time_col] >= 1600) & (df[dep_time_col] < 1900)\n",
    "        df.loc[evening_peak, 'IS_EVENING_PEAK'] = 1\n",
    "\n",
    "    weekend_count = df['IS_WEEKEND'].sum()\n",
    "    morning_peak_count = df['IS_MORNING_PEAK'].sum()\n",
    "    evening_peak_count = df['IS_EVENING_PEAK'].sum()\n",
    "    total_count = len(df)\n",
    "\n",
    "    print(f\"Identified {weekend_count} weekend flights ({weekend_count/total_count*100:.2f}%)\")\n",
    "    print(f\"Identified {morning_peak_count} morning peak flights ({morning_peak_count/total_count*100:.2f}%)\")\n",
    "    print(f\"Identified {evening_peak_count} evening peak flights ({evening_peak_count/total_count*100:.2f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to convert day of week to correct format: Sun=0, Mon=1, Tue=2, etc.\n",
    "def standardize_day_of_week(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure WEEK column exists\n",
    "    if 'WEEK' not in df.columns:\n",
    "        if 'DAY_OF_WEEK' in df.columns:\n",
    "            df['WEEK'] = df['DAY_OF_WEEK']\n",
    "        elif 'DAY' in df.columns and 'MONTH' in df.columns and 'YEAR' in df.columns:\n",
    "            if 'DATE' not in df.columns:\n",
    "                df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']])\n",
    "            df['WEEK'] = (df['DATE'].dt.dayofweek + 1) % 7\n",
    "        else:\n",
    "            print(\"Cannot create WEEK column. Required columns missing.\")\n",
    "            return df\n",
    "\n",
    "\n",
    "    if df['WEEK'].dtype == 'object':\n",
    "        day_map = {\n",
    "            'Sun': 0, 'Sunday': 0,\n",
    "            'Mon': 1, 'Monday': 1,\n",
    "            'Tue': 2, 'Tuesday': 2,\n",
    "            'Wed': 3, 'Wednesday': 3,\n",
    "            'Thu': 4, 'Thursday': 4,\n",
    "            'Fri': 5, 'Friday': 5,\n",
    "            'Sat': 6, 'Saturday': 6\n",
    "        }\n",
    "        df['WEEK'] = df['WEEK'].map(day_map)\n",
    "\n",
    "    # Convert to integer type\n",
    "    df['WEEK'] = df['WEEK'].astype(int)\n",
    "\n",
    "    # Verify the range of values is correct (0-6)\n",
    "    min_val = df['WEEK'].min()\n",
    "    max_val = df['WEEK'].max()\n",
    "\n",
    "    print(f\"Day of week range: {min_val} to {max_val} (0=Sunday, 1=Monday, ..., 6=Saturday)\")\n",
    "\n",
    "    if min_val < 0 or max_val > 6:\n",
    "        print(\"Warning: Day of week values outside expected range (0-6)\")\n",
    "\n",
    "    return df"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4 May files to process:\n",
      "  - May2021.csv\n",
      "  - May2022.csv\n",
      "  - May2023.csv\n",
      "  - May2024.csv\n",
      "\n",
      "Loading weather data...\n",
      "Found 3550 total weather data files\n",
      "Processed 100 weather files, loaded 0 matching files\n",
      "Processed 200 weather files, loaded 0 matching files\n",
      "Processed 300 weather files, loaded 16 matching files\n",
      "Processed 400 weather files, loaded 32 matching files\n",
      "Processed 500 weather files, loaded 32 matching files\n",
      "Processed 600 weather files, loaded 48 matching files\n",
      "Processed 700 weather files, loaded 48 matching files\n",
      "Processed 800 weather files, loaded 48 matching files\n",
      "Processed 900 weather files, loaded 64 matching files\n",
      "Processed 1000 weather files, loaded 64 matching files\n",
      "Processed 1100 weather files, loaded 80 matching files\n",
      "Processed 1200 weather files, loaded 96 matching files\n",
      "Processed 1300 weather files, loaded 112 matching files\n",
      "Processed 1400 weather files, loaded 112 matching files\n",
      "Processed 1500 weather files, loaded 112 matching files\n",
      "Processed 1600 weather files, loaded 112 matching files\n",
      "Processed 1700 weather files, loaded 128 matching files\n",
      "Processed 1800 weather files, loaded 128 matching files\n",
      "Processed 1900 weather files, loaded 143 matching files\n",
      "Processed 2000 weather files, loaded 160 matching files\n",
      "Processed 2100 weather files, loaded 160 matching files\n",
      "Processed 2200 weather files, loaded 192 matching files\n",
      "Processed 2300 weather files, loaded 208 matching files\n",
      "Processed 2400 weather files, loaded 208 matching files\n",
      "Processed 2500 weather files, loaded 224 matching files\n",
      "Processed 2600 weather files, loaded 240 matching files\n",
      "Processed 2700 weather files, loaded 240 matching files\n",
      "Processed 2800 weather files, loaded 256 matching files\n",
      "Processed 2900 weather files, loaded 256 matching files\n",
      "Processed 3000 weather files, loaded 272 matching files\n",
      "Processed 3100 weather files, loaded 286 matching files\n",
      "Processed 3200 weather files, loaded 306 matching files\n",
      "Processed 3300 weather files, loaded 320 matching files\n",
      "Processed 3400 weather files, loaded 336 matching files\n",
      "Processed 3500 weather files, loaded 336 matching files\n",
      "Loaded 336 weather files out of 3550 processed files\n",
      "Loading weather data took: 0.46 seconds\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T04:52:24.957795Z",
     "start_time": "2025-04-20T04:52:24.916630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to train a Random Forest model for a single CSV file\n",
    "def train_model_for_file(file_path, file_index, total_files):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    model_name = os.path.splitext(file_name)[0]\n",
    "    file_year = extract_year_from_filename(file_name)\n",
    "\n",
    "    print(f\"\\nProcessing file {file_index+1}/{total_files}: {file_name} (May {file_year})\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load flight data from this file\n",
    "    try:\n",
    "        flight_df = pd.read_csv(file_path, low_memory=False)\n",
    "        original_size = len(flight_df)\n",
    "\n",
    "        if 'MONTH' in flight_df.columns:\n",
    "            month_counts = flight_df['MONTH'].value_counts()\n",
    "            print(f\"Months found in data: {dict(month_counts)}\")\n",
    "\n",
    "            if 5 in month_counts:\n",
    "                flight_df = flight_df[flight_df['MONTH'] == 5]\n",
    "                print(f\"Filtered to only May data: {len(flight_df)} rows\")\n",
    "            else:\n",
    "                print(f\"Warning: No May data found in file, but proceeding anyway as this should be May data based on filename\")\n",
    "\n",
    "        if top_airport_codes is not None:\n",
    "            flight_df = flight_df[\n",
    "                flight_df['ORIGIN_IATA'].str.strip().isin(top_airport_codes) &\n",
    "                flight_df['DEST_IATA'].str.strip().isin(top_airport_codes)\n",
    "            ]\n",
    "\n",
    "            filtered_size = len(flight_df)\n",
    "            print(f\"Filtered from {original_size} to {filtered_size} rows for top 30 airports\")\n",
    "\n",
    "            # If no data left after filtering, skip this file\n",
    "            if filtered_size == 0:\n",
    "                print(f\"No data remaining after filtering for top 30 airports. Skipping file.\")\n",
    "                return {\n",
    "                    'file_name': file_name,\n",
    "                    'status': 'skipped',\n",
    "                    'reason': 'empty_after_filtering'\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading flight data file {file_path}: {e}\")\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'status': 'error',\n",
    "            'reason': str(e)\n",
    "        }\n",
    "\n",
    "    #print(\"Standardizing day of week...\")\n",
    "    flight_df = standardize_day_of_week(flight_df)\n",
    "\n",
    "    #print(\"Creating red-eye flight indicator...\")\n",
    "    flight_df = create_redeye_indicator(flight_df)\n",
    "\n",
    "    #print(\"Creating additional indicators: IS_WEEKEND, IS_MORNING_PEAK, IS_EVENING_PEAK...\")\n",
    "    flight_df = create_indicator_variables(flight_df)\n",
    "\n",
    "    # Basic information\n",
    "    print(f\"Final dataset shape: {flight_df.shape}\")\n",
    "\n",
    "    print(\"Preprocessing data...\")\n",
    "\n",
    "    # Create target variable\n",
    "    if 'CANCELLED' in flight_df.columns:\n",
    "        flight_df['IS_CANCELLED'] = flight_df['CANCELLED'].astype(int)\n",
    "    else:\n",
    "        print(\"No CANCELLED column found. Skipping file.\")\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'status': 'skipped',\n",
    "            'reason': 'no_cancelled_column'\n",
    "        }\n",
    "\n",
    "    cancelled_count = flight_df['IS_CANCELLED'].sum()\n",
    "    total_count = len(flight_df)\n",
    "\n",
    "    # Skip if there are no cancellations (can't train a model)\n",
    "    if cancelled_count == 0:\n",
    "        print(f\"No cancelled flights in this dataset. Skipping file.\")\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'status': 'skipped',\n",
    "            'reason': 'no_cancelled_flights'\n",
    "        }\n",
    "\n",
    "    cancellation_rate = cancelled_count / total_count * 100\n",
    "    print(f\"Overall cancellation rate: {cancelled_count}/{total_count} ({cancellation_rate:.2f}%)\")\n",
    "\n",
    "    # Analyze cancellation rates by red-eye status\n",
    "    redeye_df = flight_df[flight_df['IS_REDEYE'] == 1]\n",
    "    non_redeye_df = flight_df[flight_df['IS_REDEYE'] == 0]\n",
    "\n",
    "    if len(redeye_df) > 0:\n",
    "        redeye_cancel_rate = redeye_df['IS_CANCELLED'].mean() * 100\n",
    "        print(f\"Red-eye flights cancellation rate: {redeye_cancel_rate:.2f}%\")\n",
    "\n",
    "    if len(non_redeye_df) > 0:\n",
    "        non_redeye_cancel_rate = non_redeye_df['IS_CANCELLED'].mean() * 100\n",
    "        print(f\"Non-red-eye flights cancellation rate: {non_redeye_cancel_rate:.2f}%\")\n",
    "\n",
    "    # Analyze cancellation rates by weekend status\n",
    "    weekend_df = flight_df[flight_df['IS_WEEKEND'] == 1]\n",
    "    weekday_df = flight_df[flight_df['IS_WEEKEND'] == 0]\n",
    "\n",
    "    if len(weekend_df) > 0:\n",
    "        weekend_cancel_rate = weekend_df['IS_CANCELLED'].mean() * 100\n",
    "        print(f\"Weekend flights cancellation rate: {weekend_cancel_rate:.2f}%\")\n",
    "\n",
    "    if len(weekday_df) > 0:\n",
    "        weekday_cancel_rate = weekday_df['IS_CANCELLED'].mean() * 100\n",
    "        print(f\"Weekday flights cancellation rate: {weekday_cancel_rate:.2f}%\")\n",
    "\n",
    "    # Analyze cancellation rates by peak times\n",
    "    morning_peak_df = flight_df[flight_df['IS_MORNING_PEAK'] == 1]\n",
    "    evening_peak_df = flight_df[flight_df['IS_EVENING_PEAK'] == 1]\n",
    "    off_peak_df = flight_df[(flight_df['IS_MORNING_PEAK'] == 0) & (flight_df['IS_EVENING_PEAK'] == 0)]\n",
    "\n",
    "    if len(morning_peak_df) > 0:\n",
    "        morning_cancel_rate = morning_peak_df['IS_CANCELLED'].mean() * 100\n",
    "        print(f\"Morning peak flights cancellation rate: {morning_cancel_rate:.2f}%\")\n",
    "\n",
    "    if len(evening_peak_df) > 0:\n",
    "        evening_cancel_rate = evening_peak_df['IS_CANCELLED'].mean() * 100\n",
    "        print(f\"Evening peak flights cancellation rate: {evening_cancel_rate:.2f}%\")\n",
    "\n",
    "    if len(off_peak_df) > 0:\n",
    "        off_peak_cancel_rate = off_peak_df['IS_CANCELLED'].mean() * 100\n",
    "        print(f\"Off-peak flights cancellation rate: {off_peak_cancel_rate:.2f}%\")\n",
    "\n",
    "    print(\"Matching weather data with flights...\")\n",
    "    flight_df = match_weather_data(flight_df)\n",
    "    flight_df = match_destination_weather_data(flight_df)  # NEW: Add destination weather\n",
    "\n",
    "    # Feature selection - Now including destination weather features\n",
    "    print(\"Selecting features...\")\n",
    "\n",
    "    cat_features = [\"YEAR\", \"WEEK\", 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'IS_REDEYE',\n",
    "                     'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'EXTREME_WEATHER', 'DEST_EXTREME_WEATHER']\n",
    "    num_features = ['DISTANCE', 'PRCP', 'DEST_PRCP']\n",
    "\n",
    "    # Ensure all selected features exist in the dataframe\n",
    "    cat_features = [f for f in cat_features if f in flight_df.columns]\n",
    "    num_features = [f for f in num_features if f in flight_df.columns]\n",
    "\n",
    "    if not cat_features or not num_features:\n",
    "        print(\"Missing required features. Skipping file.\")\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'status': 'skipped',\n",
    "            'reason': 'missing_required_features'\n",
    "        }\n",
    "\n",
    "    print(f\"Using categorical features: {cat_features}\")\n",
    "    print(f\"Using numerical features: {num_features}\")\n",
    "\n",
    "    X = flight_df[cat_features + num_features].copy()\n",
    "    y = flight_df['IS_CANCELLED'].copy()\n",
    "\n",
    "    for col in cat_features:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna('unknown', inplace=True)\n",
    "    for col in num_features:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2025, stratify=y)\n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training Random Forest model...\")\n",
    "    model_start_time = time.time()\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, num_features),\n",
    "            ('cat', categorical_transformer, cat_features)\n",
    "        ])\n",
    "\n",
    "    # Create and train Random Forest model\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=250,\n",
    "            max_depth=8,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            class_weight={0: 1, 1: 5},  # Handle class imbalance\n",
    "            max_features='sqrt',\n",
    "            max_samples=0.9,        # Use 90% of samples per tree\n",
    "            random_state=2025,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    final_model = model\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    y_prob = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Feature importance from Random Forest model\n",
    "    try:\n",
    "        feature_names = final_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        rf_model = final_model.named_steps['classifier']\n",
    "        importances = rf_model.feature_importances_\n",
    "\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        })\n",
    "\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting feature importances: {e}\")\n",
    "        feature_importance = pd.DataFrame()\n",
    "\n",
    "    model_training_time = time.time() - model_start_time\n",
    "    print(f\"Model training took: {model_training_time:.2f} seconds\")\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    if not isinstance(y_pred, np.ndarray) or len(y_pred) == 0:\n",
    "        print(\"No predictions available. Skipping evaluation.\")\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'status': 'error',\n",
    "            'reason': 'prediction_failed'\n",
    "        }\n",
    "\n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        accuracy = (y_pred == y_test).mean() * 100\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        if not feature_importance.empty:\n",
    "            print(\"\\nTop 10 most important features:\")\n",
    "            print(feature_importance.head(10))\n",
    "\n",
    "            print(\"\\nImportance of indicator variables:\")\n",
    "\n",
    "            for indicator in ['IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']:\n",
    "                indicator_features = [f for f in feature_importance['Feature'].tolist() if indicator in f]\n",
    "\n",
    "                if indicator_features:\n",
    "                    for feat in indicator_features:\n",
    "                        importance = feature_importance[feature_importance['Feature'] == feat].iloc[0]['Importance']\n",
    "                        rank = feature_importance[feature_importance['Feature'] == feat].index[0] + 1\n",
    "                        print(f\"{feat} importance: {importance:.6f} (rank: {rank} out of {len(feature_importance)})\")\n",
    "\n",
    "            print(\"\\nImportance of weather features:\")\n",
    "            weather_features = [f for f in feature_importance['Feature'].tolist()\n",
    "                                if 'PRCP' in f or 'EXTREME_WEATHER' in f]\n",
    "\n",
    "            if weather_features:\n",
    "                for feat in weather_features:\n",
    "                    importance = feature_importance[feature_importance['Feature'] == feat].iloc[0]['Importance']\n",
    "                    rank = feature_importance[feature_importance['Feature'] == feat].index[0] + 1\n",
    "                    print(f\"{feat} importance: {importance:.6f} (rank: {rank} out of {len(feature_importance)})\")\n",
    "\n",
    "            # Save feature importance to CSV\n",
    "            feature_importance.to_csv(os.path.join(output_dir, 'metrics', f\"{model_name}_feature_importance.csv\"), index=False)\n",
    "\n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(16, 10))\n",
    "            top_features = feature_importance.head(15)  # Top 15 features\n",
    "            sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "            plt.title(f'Top 15 Feature Importances for {model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'plots', f\"{model_name}_feature_importance.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"Could not extract feature importances\")\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for {model_name} (Random Forest)')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, 'plots', f\"{model_name}_roc_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Not Cancelled', 'Cancelled'],\n",
    "                   yticklabels=['Not Cancelled', 'Cancelled'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Confusion Matrix for {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'plots', f\"{model_name}_confusion_matrix.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot cancellation rate comparison for all indicators\n",
    "        plt.figure(figsize=(16, 12))\n",
    "\n",
    "        # Set up the categories and their corresponding rates\n",
    "        categories = []\n",
    "        rates = []\n",
    "        counts = []\n",
    "        colors = []\n",
    "\n",
    "        # Add red-eye vs non-red-eye\n",
    "        if len(redeye_df) > 0 and len(non_redeye_df) > 0:\n",
    "            categories.extend(['Non-Red-Eye', 'Red-Eye'])\n",
    "            rates.extend([non_redeye_cancel_rate, redeye_cancel_rate])\n",
    "            counts.extend([len(non_redeye_df), len(redeye_df)])\n",
    "            colors.extend(['skyblue', 'navy'])\n",
    "\n",
    "        # Add weekend vs weekday\n",
    "        if len(weekend_df) > 0 and len(weekday_df) > 0:\n",
    "            categories.extend(['Weekday', 'Weekend'])\n",
    "            rates.extend([weekday_cancel_rate, weekend_cancel_rate])\n",
    "            counts.extend([len(weekday_df), len(weekend_df)])\n",
    "            colors.extend(['lightgreen', 'darkgreen'])\n",
    "\n",
    "        # Add peak times\n",
    "        if len(morning_peak_df) > 0 and len(evening_peak_df) > 0 and len(off_peak_df) > 0:\n",
    "            categories.extend(['Off-Peak', 'Morning Peak', 'Evening Peak'])\n",
    "            rates.extend([off_peak_cancel_rate, morning_cancel_rate, evening_cancel_rate])\n",
    "            counts.extend([len(off_peak_df), len(morning_peak_df), len(evening_peak_df)])\n",
    "            colors.extend(['lightcoral', 'salmon', 'firebrick'])\n",
    "\n",
    "        # Create the bar chart\n",
    "        plt.figure(figsize=(18, 10))\n",
    "        bars = plt.bar(categories, rates, color=colors)\n",
    "\n",
    "        # Add value labels on top of bars\n",
    "        for i, (bar, rate, count) in enumerate(zip(bars, rates, counts)):\n",
    "            plt.text(i, rate + 0.5, f\"{rate:.2f}%\\n({count} flights)\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "        plt.ylabel('Cancellation Rate (%)')\n",
    "        plt.title(f'Cancellation Rate Comparison ({model_name})')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'plots', f\"{model_name}_indicator_comparison.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Generate partial dependence plots for indicators\n",
    "        try:\n",
    "            indicator_effects = {}\n",
    "            for indicator in ['IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']:\n",
    "                if indicator in X_test.columns:\n",
    "\n",
    "                    positive_X_test = X_test.copy()\n",
    "                    positive_X_test[indicator] = 1\n",
    "\n",
    "                    negative_X_test = X_test.copy()\n",
    "                    negative_X_test[indicator] = 0\n",
    "\n",
    "                    positive_probs = final_model.predict_proba(positive_X_test)[:, 1]\n",
    "                    negative_probs = final_model.predict_proba(negative_X_test)[:, 1]\n",
    "\n",
    "                    avg_effect = np.mean(positive_probs - negative_probs)\n",
    "\n",
    "                    # Store the effect\n",
    "                    indicator_effects[indicator] = {\n",
    "                        'avg_effect': avg_effect,\n",
    "                        'differences': positive_probs - negative_probs\n",
    "                    }\n",
    "\n",
    "                    # Plot distribution of effects for this indicator\n",
    "                    plt.figure(figsize=(16, 10))\n",
    "\n",
    "                    # Plot the differences for each sample\n",
    "                    differences = positive_probs - negative_probs\n",
    "                    sns.histplot(differences, bins=30, kde=True)\n",
    "\n",
    "                    plt.axvline(avg_effect, color='red', linestyle='--',\n",
    "                               label=f'Average effect: {avg_effect:.4f}')\n",
    "                    plt.xlabel(f'Change in Cancellation Probability ({indicator}=1 vs {indicator}=0)')\n",
    "                    plt.ylabel('Frequency')\n",
    "                    plt.title(f'Effect of {indicator} on Cancellation Probability ({model_name})')\n",
    "                    plt.legend()\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(output_dir, 'plots', f\"{model_name}_{indicator}_effect.png\"))\n",
    "                    plt.close()\n",
    "\n",
    "                    print(f\"Average effect of {indicator}=1 on cancellation probability: {avg_effect:.4f}\")\n",
    "\n",
    "            # Create a comparison of all indicator effects\n",
    "            if indicator_effects:\n",
    "                plt.figure(figsize=(16, 10))\n",
    "                indicators = list(indicator_effects.keys())\n",
    "                effects = [indicator_effects[ind]['avg_effect'] for ind in indicators]\n",
    "\n",
    "                colors = ['red' if e > 0 else 'green' for e in effects]\n",
    "\n",
    "                bars = plt.bar(indicators, effects, color=colors)\n",
    "\n",
    "                # Add value labels\n",
    "                for i, (bar, effect) in enumerate(zip(bars, effects)):\n",
    "                    plt.text(i, effect + 0.001 if effect > 0 else effect - 0.003,\n",
    "                             f\"{effect:.4f}\", ha='center', va='center' if effect > 0 else 'top')\n",
    "\n",
    "                plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "                plt.ylabel('Average Effect on Cancellation Probability')\n",
    "                plt.title(f'Comparison of Indicator Effects on Cancellation Probability ({model_name})')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'plots', f\"{model_name}_all_indicators_effect.png\"))\n",
    "                plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating indicator effect plots: {e}\")\n",
    "\n",
    "        # Save model\n",
    "        model_path = os.path.join(output_dir, f\"{model_name}_model.joblib\")\n",
    "        dump(final_model, model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Save metrics summary\n",
    "        metrics = {\n",
    "            'file_name': file_name,\n",
    "            'model_name': model_name,\n",
    "            'file_year': file_year,\n",
    "            'accuracy': accuracy,\n",
    "            'roc_auc': roc_auc,\n",
    "            'precision': report['1']['precision'],\n",
    "            'recall': report['1']['recall'],\n",
    "            'f1_score': report['1']['f1-score'],\n",
    "            'cancellation_rate': cancellation_rate,\n",
    "            'training_time': model_training_time,\n",
    "            'training_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'status': 'success'\n",
    "        }\n",
    "\n",
    "        # Add indicator-specific metrics\n",
    "        metrics['redeye_count'] = len(redeye_df)\n",
    "        metrics['redeye_percentage'] = len(redeye_df) / len(flight_df) * 100\n",
    "        metrics['redeye_cancel_rate'] = redeye_cancel_rate if len(redeye_df) > 0 else None\n",
    "        metrics['non_redeye_cancel_rate'] = non_redeye_cancel_rate if len(non_redeye_df) > 0 else None\n",
    "\n",
    "        # Weekend metrics\n",
    "        metrics['weekend_count'] = len(weekend_df)\n",
    "        metrics['weekend_percentage'] = len(weekend_df) / len(flight_df) * 100\n",
    "        metrics['weekend_cancel_rate'] = weekend_cancel_rate if len(weekend_df) > 0 else None\n",
    "        metrics['weekday_cancel_rate'] = weekday_cancel_rate if len(weekday_df) > 0 else None\n",
    "\n",
    "        # Peak time metrics\n",
    "        metrics['morning_peak_count'] = len(morning_peak_df)\n",
    "        metrics['morning_peak_percentage'] = len(morning_peak_df) / len(flight_df) * 100\n",
    "        metrics['morning_peak_cancel_rate'] = morning_cancel_rate if len(morning_peak_df) > 0 else None\n",
    "\n",
    "        metrics['evening_peak_count'] = len(evening_peak_df)\n",
    "        metrics['evening_peak_percentage'] = len(evening_peak_df) / len(flight_df) * 100\n",
    "        metrics['evening_peak_cancel_rate'] = evening_cancel_rate if len(evening_peak_df) > 0 else None\n",
    "\n",
    "        metrics['off_peak_count'] = len(off_peak_df)\n",
    "        metrics['off_peak_percentage'] = len(off_peak_df) / len(flight_df) * 100\n",
    "        metrics['off_peak_cancel_rate'] = off_peak_cancel_rate if len(off_peak_df) > 0 else None\n",
    "\n",
    "        # Feature importance for indicators\n",
    "        for indicator in ['IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']:\n",
    "            indicator_features = [f for f in feature_importance['Feature'].tolist() if indicator in f]\n",
    "            if indicator_features:\n",
    "                for feat in indicator_features:\n",
    "                    importance = feature_importance[feature_importance['Feature'] == feat].iloc[0]['Importance']\n",
    "                    metrics[f'{indicator.lower()}_importance'] = importance\n",
    "\n",
    "        # Feature importance for weather features - now including destination weather\n",
    "        weather_features = [f for f in feature_importance['Feature'].tolist()\n",
    "                           if 'PRCP' in f or 'EXTREME_WEATHER' in f]\n",
    "        if weather_features:\n",
    "            for feat in weather_features:\n",
    "                importance = feature_importance[feature_importance['Feature'] == feat].iloc[0]['Importance']\n",
    "                feat_key = feat.replace('cat__', '').replace('num__', '')\n",
    "                metrics[f'{feat_key}_importance'] = importance\n",
    "\n",
    "        # Add indicator effect from partial dependence if available\n",
    "        if 'indicator_effects' in locals():\n",
    "            for indicator, effect_data in indicator_effects.items():\n",
    "                metrics[f'{indicator.lower()}_effect'] = effect_data['avg_effect']\n",
    "\n",
    "        # Save confusion matrix values\n",
    "        metrics['true_negative'] = cm[0, 0]\n",
    "        metrics['false_positive'] = cm[0, 1]\n",
    "        metrics['false_negative'] = cm[1, 0]\n",
    "        metrics['true_positive'] = cm[1, 1]\n",
    "\n",
    "        # Save top 5 most important features\n",
    "        if not feature_importance.empty:\n",
    "            for i in range(min(5, len(feature_importance))):\n",
    "                feat = feature_importance.iloc[i]\n",
    "                metrics[f'top_feature_{i+1}'] = feat['Feature']\n",
    "                metrics[f'top_feature_{i+1}_importance'] = feat['Importance']\n",
    "\n",
    "        # Add weather data match rates\n",
    "        origin_matched = flight_df['PRCP'].notnull().sum()\n",
    "        dest_matched = flight_df['DEST_PRCP'].notnull().sum() if 'DEST_PRCP' in flight_df.columns else 0\n",
    "\n",
    "        metrics['origin_weather_match_rate'] = origin_matched / len(flight_df) * 100\n",
    "        metrics['dest_weather_match_rate'] = dest_matched / len(flight_df) * 100\n",
    "\n",
    "        print(f\"Processing of {file_name} completed in {time.time() - start_time:.2f} seconds\")\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation: {e}\")\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'status': 'error',\n",
    "            'reason': str(e)\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T05:03:02.723641Z",
     "start_time": "2025-04-20T04:52:24.980878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Sequential processing of the May files\n",
    "results = []\n",
    "for i, file_path in enumerate(flight_files):\n",
    "    result = train_model_for_file(file_path, i, len(flight_files))\n",
    "    results.append(result)\n",
    "\n",
    "# Summarize results\n",
    "print(\"\\nSummary of Random Forest model training:\")\n",
    "success_count = sum(1 for r in results if r.get('status') == 'success')\n",
    "error_count = sum(1 for r in results if r.get('status') == 'error')\n",
    "skipped_count = sum(1 for r in results if r.get('status') == 'skipped')\n",
    "\n",
    "print(f\"Successfully trained models: {success_count}/{len(results)}\")\n",
    "print(f\"Failed models: {error_count}/{len(results)}\")\n",
    "print(f\"Skipped files: {skipped_count}/{len(results)}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(output_dir, 'cancelled_prob_rf_summary.csv'), index=False)\n",
    "\n",
    "# Calculate average metrics\n",
    "if success_count > 0:\n",
    "    successful_results = [r for r in results if r.get('status') == 'success']\n",
    "    avg_accuracy = sum(r.get('accuracy', 0) for r in successful_results) / success_count\n",
    "    avg_roc_auc = sum(r.get('roc_auc', 0) for r in successful_results) / success_count\n",
    "    avg_precision = sum(r.get('precision', 0) for r in successful_results) / success_count\n",
    "    avg_recall = sum(r.get('recall', 0) for r in successful_results) / success_count\n",
    "    avg_origin_weather_match = sum(r.get('origin_weather_match_rate', 0) for r in successful_results) / success_count if any('origin_weather_match_rate' in r for r in successful_results) else 0\n",
    "    avg_dest_weather_match = sum(r.get('dest_weather_match_rate', 0) for r in successful_results) / success_count if any('dest_weather_match_rate' in r for r in successful_results) else 0\n",
    "\n",
    "    print(\"\\nAverage metrics across all successful models:\")\n",
    "    print(f\"Accuracy: {avg_accuracy:.2f}%\")\n",
    "    print(f\"ROC AUC: {avg_roc_auc:.4f}\")\n",
    "    print(f\"Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Origin weather data match rate: {avg_origin_weather_match:.2f}%\")\n",
    "    print(f\"Destination weather data match rate: {avg_dest_weather_match:.2f}%\")\n",
    "\n",
    "    # For successful models, identify most common important features\n",
    "    feature_counts = {}\n",
    "    for result in successful_results:\n",
    "        for i in range(1, 6):  # Top 5 features\n",
    "            feature_key = f'top_feature_{i}'\n",
    "            if feature_key in result:\n",
    "                feature = result[feature_key]\n",
    "                if feature in feature_counts:\n",
    "                    feature_counts[feature] += 1\n",
    "                else:\n",
    "                    feature_counts[feature] = 1\n",
    "\n",
    "    if feature_counts:\n",
    "        print(\"\\nMost common important features across all models:\")\n",
    "        sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for feature, count in sorted_features[:10]:  # Top 10 most common\n",
    "            print(f\"{feature}: Appears in {count} models ({count/success_count*100:.1f}%)\")\n",
    "\n",
    "    # Analyze all indicator effects across all models\n",
    "    indicators = ['is_redeye', 'is_weekend', 'is_morning_peak', 'is_evening_peak']\n",
    "    for indicator in indicators:\n",
    "        effect_key = f'{indicator}_effect'\n",
    "        importance_key = f'{indicator}_importance'\n",
    "\n",
    "        if all(effect_key in r for r in successful_results):\n",
    "            effects = [r[effect_key] for r in successful_results]\n",
    "            avg_effect = sum(effects) / len(effects)\n",
    "\n",
    "            effect_dir = \"increases\" if avg_effect > 0 else \"decreases\"\n",
    "            print(f\"\\n{indicator.upper()} generally {effect_dir} cancellation probability by {abs(avg_effect):.4f}\")\n",
    "\n",
    "        if all(importance_key in r for r in successful_results):\n",
    "            importances = [r[importance_key] for r in successful_results]\n",
    "            avg_importance = sum(importances) / len(importances)\n",
    "\n",
    "            print(f\"Average {indicator.upper()} importance: {avg_importance:.6f}\")\n",
    "\n",
    "    # Analyze all weather feature importance across all models\n",
    "    weather_indicators = ['prcp', 'extreme_weather', 'dest_prcp', 'dest_extreme_weather']\n",
    "    for indicator in weather_indicators:\n",
    "        importance_key = f'{indicator}_importance'\n",
    "\n",
    "        if any(importance_key in r for r in successful_results):\n",
    "            results_with_feature = [r for r in successful_results if importance_key in r]\n",
    "            if results_with_feature:\n",
    "                importances = [r[importance_key] for r in results_with_feature]\n",
    "                avg_importance = sum(importances) / len(importances)\n",
    "\n",
    "                print(f\"Average {indicator.upper()} importance: {avg_importance:.6f} (found in {len(results_with_feature)}/{success_count} models)\")\n",
    "\n",
    "    # Plot combined indicator effects across all models\n",
    "    all_indicators = []\n",
    "    all_effects = []\n",
    "\n",
    "    for indicator in indicators:\n",
    "        effect_key = f'{indicator}_effect'\n",
    "        if all(effect_key in r for r in successful_results):\n",
    "            all_indicators.append(indicator.upper())\n",
    "            effects = [r[effect_key] for r in successful_results]\n",
    "            avg_effect = sum(effects) / len(effects)\n",
    "            all_effects.append(avg_effect)\n",
    "\n",
    "    if all_indicators:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "\n",
    "        # Determine colors based on effect direction\n",
    "        colors = ['red' if e > 0 else 'green' for e in all_effects]\n",
    "\n",
    "        bars = plt.bar(all_indicators, all_effects, color=colors)\n",
    "\n",
    "        # Add value labels\n",
    "        for i, (bar, effect) in enumerate(zip(bars, all_effects)):\n",
    "            plt.text(i, effect + 0.001 if effect > 0 else effect - 0.003,\n",
    "                     f\"{effect:.4f}\", ha='center', va='center' if effect > 0 else 'top')\n",
    "\n",
    "        plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        plt.ylabel('Average Effect on Cancellation Probability')\n",
    "        plt.title('Average Indicator Effects Across All Models')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'plots', 'average_all_indicators_effect.png'))\n",
    "        plt.close()\n",
    "        print(\"\\nAverage indicator effects comparison saved to average_all_indicators_effect.png\")\n",
    "\n",
    "print(\"\\nRandom Forest model training with enhanced indicators and destination weather complete!\")\n",
    "print(f\"Full summary saved to {os.path.join(output_dir, 'cancelled_prob_rf_summary.csv')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file 1/4: May2021.csv (May 2021)\n",
      "Months found in data: {5: 520059}\n",
      "Filtered to only May data: 520059 rows\n",
      "Filtered from 520059 to 171867 rows for top 30 airports\n",
      "Standardizing day of week...\n",
      "Day of week range: 0 to 6 (0=Sunday, 1=Monday, ..., 6=Saturday)\n",
      "Creating red-eye flight indicator...\n",
      "Identified 7202 red-eye flights out of 171867 total flights (4.19%)\n",
      "Creating additional indicators: IS_WEEKEND, IS_MORNING_PEAK, IS_EVENING_PEAK...\n",
      "Identified 54383 weekend flights (31.64%)\n",
      "Identified 37460 morning peak flights (21.80%)\n",
      "Identified 30873 evening peak flights (17.96%)\n",
      "Final dataset shape: (171867, 55)\n",
      "Preprocessing data...\n",
      "Overall cancellation rate: 485/171867 (0.28%)\n",
      "Red-eye flights cancellation rate: 0.22%\n",
      "Non-red-eye flights cancellation rate: 0.28%\n",
      "Weekend flights cancellation rate: 0.15%\n",
      "Weekday flights cancellation rate: 0.34%\n",
      "Morning peak flights cancellation rate: 0.25%\n",
      "Evening peak flights cancellation rate: 0.35%\n",
      "Off-peak flights cancellation rate: 0.27%\n",
      "Matching weather data with flights...\n",
      "\n",
      "Matching origin weather data with flights...\n",
      "Processed 10000/171867 rows, matched 7631 flights with origin weather data\n",
      "Processed 20000/171867 rows, matched 15252 flights with origin weather data\n",
      "Processed 30000/171867 rows, matched 22810 flights with origin weather data\n",
      "Processed 40000/171867 rows, matched 30379 flights with origin weather data\n",
      "Processed 50000/171867 rows, matched 37897 flights with origin weather data\n",
      "Processed 60000/171867 rows, matched 45547 flights with origin weather data\n",
      "Processed 70000/171867 rows, matched 53228 flights with origin weather data\n",
      "Processed 80000/171867 rows, matched 60768 flights with origin weather data\n",
      "Processed 90000/171867 rows, matched 68319 flights with origin weather data\n",
      "Processed 100000/171867 rows, matched 75910 flights with origin weather data\n",
      "Processed 110000/171867 rows, matched 83572 flights with origin weather data\n",
      "Processed 120000/171867 rows, matched 91187 flights with origin weather data\n",
      "Processed 130000/171867 rows, matched 98675 flights with origin weather data\n",
      "Processed 140000/171867 rows, matched 106268 flights with origin weather data\n",
      "Processed 150000/171867 rows, matched 113913 flights with origin weather data\n",
      "Processed 160000/171867 rows, matched 121588 flights with origin weather data\n",
      "Processed 170000/171867 rows, matched 129123 flights with origin weather data\n",
      "Processed 171867/171867 rows, matched 130672 flights with origin weather data\n",
      "Matched origin weather data for 130672 flights (76.03%)\n",
      "Origin weather matching took: 62.04 seconds\n",
      "\n",
      "Matching destination weather data with flights...\n",
      "Processed 10000/171867 rows, matched 7617 flights with destination weather data\n",
      "Processed 20000/171867 rows, matched 15255 flights with destination weather data\n",
      "Processed 30000/171867 rows, matched 22828 flights with destination weather data\n",
      "Processed 40000/171867 rows, matched 30397 flights with destination weather data\n",
      "Processed 50000/171867 rows, matched 37923 flights with destination weather data\n",
      "Processed 60000/171867 rows, matched 45557 flights with destination weather data\n",
      "Processed 70000/171867 rows, matched 53246 flights with destination weather data\n",
      "Processed 80000/171867 rows, matched 60786 flights with destination weather data\n",
      "Processed 90000/171867 rows, matched 68337 flights with destination weather data\n",
      "Processed 100000/171867 rows, matched 75929 flights with destination weather data\n",
      "Processed 110000/171867 rows, matched 83580 flights with destination weather data\n",
      "Processed 120000/171867 rows, matched 91196 flights with destination weather data\n",
      "Processed 130000/171867 rows, matched 98681 flights with destination weather data\n",
      "Processed 140000/171867 rows, matched 106278 flights with destination weather data\n",
      "Processed 150000/171867 rows, matched 113930 flights with destination weather data\n",
      "Processed 160000/171867 rows, matched 121613 flights with destination weather data\n",
      "Processed 170000/171867 rows, matched 129139 flights with destination weather data\n",
      "Processed 171867/171867 rows, matched 130684 flights with destination weather data\n",
      "Matched destination weather data for 130684 flights (76.04%)\n",
      "Destination weather matching took: 62.88 seconds\n",
      "Selecting features...\n",
      "Using categorical features: ['YEAR', 'WEEK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'EXTREME_WEATHER', 'DEST_EXTREME_WEATHER']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'DEST_PRCP']\n",
      "Training set size: (137493, 14)\n",
      "Test set size: (34374, 14)\n",
      "Training Random Forest model...\n",
      "Model training took: 3.94 seconds\n",
      "Evaluating model...\n",
      "Accuracy: 99.72%\n",
      "ROC AUC: 0.8598\n",
      "\n",
      "Top 10 most important features:\n",
      "                 Feature  Importance\n",
      "28  cat__ORIGIN_IATA_DFW    0.133964\n",
      "6            cat__WEEK_2    0.107956\n",
      "58    cat__DEST_IATA_DFW    0.107737\n",
      "0          num__DISTANCE    0.085156\n",
      "11   cat__MKT_AIRLINE_AA    0.066512\n",
      "1              num__PRCP    0.062978\n",
      "2         num__DEST_PRCP    0.048137\n",
      "5            cat__WEEK_1    0.027987\n",
      "14   cat__MKT_AIRLINE_DL    0.027939\n",
      "9            cat__WEEK_5    0.015738\n",
      "\n",
      "Importance of indicator variables:\n",
      "cat__IS_REDEYE_0 importance: 0.002177 (rank: 81 out of 92)\n",
      "cat__IS_REDEYE_1 importance: 0.001933 (rank: 82 out of 92)\n",
      "cat__IS_WEEKEND_0 importance: 0.015325 (rank: 83 out of 92)\n",
      "cat__IS_WEEKEND_1 importance: 0.012593 (rank: 84 out of 92)\n",
      "cat__IS_MORNING_PEAK_0 importance: 0.014983 (rank: 85 out of 92)\n",
      "cat__IS_MORNING_PEAK_1 importance: 0.013090 (rank: 86 out of 92)\n",
      "cat__IS_EVENING_PEAK_0 importance: 0.013465 (rank: 87 out of 92)\n",
      "cat__IS_EVENING_PEAK_1 importance: 0.011119 (rank: 88 out of 92)\n",
      "\n",
      "Importance of weather features:\n",
      "num__PRCP importance: 0.062978 (rank: 2 out of 92)\n",
      "num__DEST_PRCP importance: 0.048137 (rank: 3 out of 92)\n",
      "cat__EXTREME_WEATHER_1.0 importance: 0.012467 (rank: 90 out of 92)\n",
      "cat__EXTREME_WEATHER_0.0 importance: 0.010834 (rank: 89 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_1.0 importance: 0.010167 (rank: 92 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_0.0 importance: 0.009543 (rank: 91 out of 92)\n",
      "Average effect of IS_REDEYE=1 on cancellation probability: 0.0002\n",
      "Average effect of IS_WEEKEND=1 on cancellation probability: -0.0041\n",
      "Average effect of IS_MORNING_PEAK=1 on cancellation probability: -0.0005\n",
      "Average effect of IS_EVENING_PEAK=1 on cancellation probability: 0.0012\n",
      "Model saved to ./cancelled_prob_rf_models/May2021_model.joblib\n",
      "Processing of May2021.csv completed in 135.25 seconds\n",
      "\n",
      "Processing file 2/4: May2022.csv (May 2022)\n",
      "Months found in data: {5: 602950}\n",
      "Filtered to only May data: 602950 rows\n",
      "Filtered from 602950 to 210079 rows for top 30 airports\n",
      "Standardizing day of week...\n",
      "Day of week range: 0 to 6 (0=Sunday, 1=Monday, ..., 6=Saturday)\n",
      "Creating red-eye flight indicator...\n",
      "Identified 13296 red-eye flights out of 210079 total flights (6.33%)\n",
      "Creating additional indicators: IS_WEEKEND, IS_MORNING_PEAK, IS_EVENING_PEAK...\n",
      "Identified 58134 weekend flights (27.67%)\n",
      "Identified 43979 morning peak flights (20.93%)\n",
      "Identified 37073 evening peak flights (17.65%)\n",
      "Final dataset shape: (210079, 54)\n",
      "Preprocessing data...\n",
      "Overall cancellation rate: 4659/210079 (2.22%)\n",
      "Red-eye flights cancellation rate: 1.56%\n",
      "Non-red-eye flights cancellation rate: 2.26%\n",
      "Weekend flights cancellation rate: 3.09%\n",
      "Weekday flights cancellation rate: 1.88%\n",
      "Morning peak flights cancellation rate: 1.87%\n",
      "Evening peak flights cancellation rate: 2.83%\n",
      "Off-peak flights cancellation rate: 2.16%\n",
      "Matching weather data with flights...\n",
      "\n",
      "Matching origin weather data with flights...\n",
      "Processed 10000/210079 rows, matched 7052 flights with origin weather data\n",
      "Processed 20000/210079 rows, matched 14364 flights with origin weather data\n",
      "Processed 30000/210079 rows, matched 21382 flights with origin weather data\n",
      "Processed 40000/210079 rows, matched 28684 flights with origin weather data\n",
      "Processed 50000/210079 rows, matched 35884 flights with origin weather data\n",
      "Processed 60000/210079 rows, matched 43206 flights with origin weather data\n",
      "Processed 70000/210079 rows, matched 50223 flights with origin weather data\n",
      "Processed 80000/210079 rows, matched 57533 flights with origin weather data\n",
      "Processed 90000/210079 rows, matched 64770 flights with origin weather data\n",
      "Processed 100000/210079 rows, matched 72052 flights with origin weather data\n",
      "Processed 110000/210079 rows, matched 79238 flights with origin weather data\n",
      "Processed 120000/210079 rows, matched 86376 flights with origin weather data\n",
      "Processed 130000/210079 rows, matched 93671 flights with origin weather data\n",
      "Processed 140000/210079 rows, matched 100878 flights with origin weather data\n",
      "Processed 150000/210079 rows, matched 108190 flights with origin weather data\n",
      "Processed 160000/210079 rows, matched 115277 flights with origin weather data\n",
      "Processed 170000/210079 rows, matched 122635 flights with origin weather data\n",
      "Processed 180000/210079 rows, matched 129736 flights with origin weather data\n",
      "Processed 190000/210079 rows, matched 137136 flights with origin weather data\n",
      "Processed 200000/210079 rows, matched 144229 flights with origin weather data\n",
      "Processed 210000/210079 rows, matched 151383 flights with origin weather data\n",
      "Processed 210079/210079 rows, matched 151446 flights with origin weather data\n",
      "Matched origin weather data for 151446 flights (72.09%)\n",
      "Origin weather matching took: 71.54 seconds\n",
      "\n",
      "Matching destination weather data with flights...\n",
      "Processed 10000/210079 rows, matched 7036 flights with destination weather data\n",
      "Processed 20000/210079 rows, matched 14349 flights with destination weather data\n",
      "Processed 30000/210079 rows, matched 21349 flights with destination weather data\n",
      "Processed 40000/210079 rows, matched 28668 flights with destination weather data\n",
      "Processed 50000/210079 rows, matched 35864 flights with destination weather data\n",
      "Processed 60000/210079 rows, matched 43199 flights with destination weather data\n",
      "Processed 70000/210079 rows, matched 50207 flights with destination weather data\n",
      "Processed 80000/210079 rows, matched 57512 flights with destination weather data\n",
      "Processed 90000/210079 rows, matched 64750 flights with destination weather data\n",
      "Processed 100000/210079 rows, matched 72034 flights with destination weather data\n",
      "Processed 110000/210079 rows, matched 79227 flights with destination weather data\n",
      "Processed 120000/210079 rows, matched 86361 flights with destination weather data\n",
      "Processed 130000/210079 rows, matched 93639 flights with destination weather data\n",
      "Processed 140000/210079 rows, matched 100867 flights with destination weather data\n",
      "Processed 150000/210079 rows, matched 108157 flights with destination weather data\n",
      "Processed 160000/210079 rows, matched 115264 flights with destination weather data\n",
      "Processed 170000/210079 rows, matched 122627 flights with destination weather data\n",
      "Processed 180000/210079 rows, matched 129711 flights with destination weather data\n",
      "Processed 190000/210079 rows, matched 137128 flights with destination weather data\n",
      "Processed 200000/210079 rows, matched 144212 flights with destination weather data\n",
      "Processed 210000/210079 rows, matched 151372 flights with destination weather data\n",
      "Processed 210079/210079 rows, matched 151436 flights with destination weather data\n",
      "Matched destination weather data for 151436 flights (72.09%)\n",
      "Destination weather matching took: 71.65 seconds\n",
      "Selecting features...\n",
      "Using categorical features: ['YEAR', 'WEEK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'EXTREME_WEATHER', 'DEST_EXTREME_WEATHER']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'DEST_PRCP']\n",
      "Training set size: (168063, 14)\n",
      "Test set size: (42016, 14)\n",
      "Training Random Forest model...\n",
      "Model training took: 5.73 seconds\n",
      "Evaluating model...\n",
      "Accuracy: 97.78%\n",
      "ROC AUC: 0.7844\n",
      "\n",
      "Top 10 most important features:\n",
      "                 Feature  Importance\n",
      "9            cat__WEEK_5    0.096442\n",
      "0          num__DISTANCE    0.079967\n",
      "14   cat__MKT_AIRLINE_DL    0.062816\n",
      "7            cat__WEEK_3    0.062511\n",
      "2         num__DEST_PRCP    0.049846\n",
      "60    cat__DEST_IATA_EWR    0.049242\n",
      "1              num__PRCP    0.046108\n",
      "8            cat__WEEK_4    0.045789\n",
      "67    cat__DEST_IATA_LGA    0.035834\n",
      "37  cat__ORIGIN_IATA_LGA    0.035173\n",
      "\n",
      "Importance of indicator variables:\n",
      "cat__IS_REDEYE_0 importance: 0.002227 (rank: 81 out of 92)\n",
      "cat__IS_REDEYE_1 importance: 0.002065 (rank: 82 out of 92)\n",
      "cat__IS_WEEKEND_1 importance: 0.031649 (rank: 84 out of 92)\n",
      "cat__IS_WEEKEND_0 importance: 0.029670 (rank: 83 out of 92)\n",
      "cat__IS_MORNING_PEAK_1 importance: 0.013232 (rank: 86 out of 92)\n",
      "cat__IS_MORNING_PEAK_0 importance: 0.011692 (rank: 85 out of 92)\n",
      "cat__IS_EVENING_PEAK_1 importance: 0.017077 (rank: 88 out of 92)\n",
      "cat__IS_EVENING_PEAK_0 importance: 0.015340 (rank: 87 out of 92)\n",
      "\n",
      "Importance of weather features:\n",
      "num__DEST_PRCP importance: 0.049846 (rank: 3 out of 92)\n",
      "num__PRCP importance: 0.046108 (rank: 2 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_1.0 importance: 0.008785 (rank: 92 out of 92)\n",
      "cat__EXTREME_WEATHER_0.0 importance: 0.008270 (rank: 89 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_0.0 importance: 0.008127 (rank: 91 out of 92)\n",
      "cat__EXTREME_WEATHER_1.0 importance: 0.007632 (rank: 90 out of 92)\n",
      "Average effect of IS_REDEYE=1 on cancellation probability: -0.0005\n",
      "Average effect of IS_WEEKEND=1 on cancellation probability: 0.0263\n",
      "Average effect of IS_MORNING_PEAK=1 on cancellation probability: -0.0050\n",
      "Average effect of IS_EVENING_PEAK=1 on cancellation probability: 0.0097\n",
      "Model saved to ./cancelled_prob_rf_models/May2022_model.joblib\n",
      "Processing of May2022.csv completed in 155.97 seconds\n",
      "\n",
      "Processing file 3/4: May2023.csv (May 2023)\n",
      "Months found in data: {5: 616630}\n",
      "Filtered to only May data: 616630 rows\n",
      "Filtered from 616630 to 220469 rows for top 30 airports\n",
      "Standardizing day of week...\n",
      "Day of week range: 0 to 6 (0=Sunday, 1=Monday, ..., 6=Saturday)\n",
      "Creating red-eye flight indicator...\n",
      "Identified 15208 red-eye flights out of 220469 total flights (6.90%)\n",
      "Creating additional indicators: IS_WEEKEND, IS_MORNING_PEAK, IS_EVENING_PEAK...\n",
      "Identified 53634 weekend flights (24.33%)\n",
      "Identified 45234 morning peak flights (20.52%)\n",
      "Identified 39099 evening peak flights (17.73%)\n",
      "Final dataset shape: (220469, 54)\n",
      "Preprocessing data...\n",
      "Overall cancellation rate: 1293/220469 (0.59%)\n",
      "Red-eye flights cancellation rate: 0.74%\n",
      "Non-red-eye flights cancellation rate: 0.57%\n",
      "Weekend flights cancellation rate: 0.72%\n",
      "Weekday flights cancellation rate: 0.54%\n",
      "Morning peak flights cancellation rate: 0.53%\n",
      "Evening peak flights cancellation rate: 0.67%\n",
      "Off-peak flights cancellation rate: 0.58%\n",
      "Matching weather data with flights...\n",
      "\n",
      "Matching origin weather data with flights...\n",
      "Processed 10000/220469 rows, matched 7036 flights with origin weather data\n",
      "Processed 20000/220469 rows, matched 14349 flights with origin weather data\n",
      "Processed 30000/220469 rows, matched 21637 flights with origin weather data\n",
      "Processed 40000/220469 rows, matched 28891 flights with origin weather data\n",
      "Processed 50000/220469 rows, matched 36382 flights with origin weather data\n",
      "Processed 60000/220469 rows, matched 43507 flights with origin weather data\n",
      "Processed 70000/220469 rows, matched 50847 flights with origin weather data\n",
      "Processed 80000/220469 rows, matched 58190 flights with origin weather data\n",
      "Processed 90000/220469 rows, matched 65446 flights with origin weather data\n",
      "Processed 100000/220469 rows, matched 72920 flights with origin weather data\n",
      "Processed 110000/220469 rows, matched 80014 flights with origin weather data\n",
      "Processed 120000/220469 rows, matched 86944 flights with origin weather data\n",
      "Processed 130000/220469 rows, matched 93854 flights with origin weather data\n",
      "Processed 140000/220469 rows, matched 101155 flights with origin weather data\n",
      "Processed 150000/220469 rows, matched 108639 flights with origin weather data\n",
      "Processed 160000/220469 rows, matched 115812 flights with origin weather data\n",
      "Processed 170000/220469 rows, matched 123155 flights with origin weather data\n",
      "Processed 180000/220469 rows, matched 130470 flights with origin weather data\n",
      "Processed 190000/220469 rows, matched 137794 flights with origin weather data\n",
      "Processed 200000/220469 rows, matched 145238 flights with origin weather data\n",
      "Processed 210000/220469 rows, matched 152378 flights with origin weather data\n",
      "Processed 220000/220469 rows, matched 159909 flights with origin weather data\n",
      "Processed 220469/220469 rows, matched 160290 flights with origin weather data\n",
      "Matched origin weather data for 160290 flights (72.70%)\n",
      "Origin weather matching took: 78.97 seconds\n",
      "\n",
      "Matching destination weather data with flights...\n",
      "Processed 10000/220469 rows, matched 7032 flights with destination weather data\n",
      "Processed 20000/220469 rows, matched 14339 flights with destination weather data\n",
      "Processed 30000/220469 rows, matched 21644 flights with destination weather data\n",
      "Processed 40000/220469 rows, matched 28890 flights with destination weather data\n",
      "Processed 50000/220469 rows, matched 36381 flights with destination weather data\n",
      "Processed 60000/220469 rows, matched 43505 flights with destination weather data\n",
      "Processed 70000/220469 rows, matched 50839 flights with destination weather data\n",
      "Processed 80000/220469 rows, matched 58180 flights with destination weather data\n",
      "Processed 90000/220469 rows, matched 65439 flights with destination weather data\n",
      "Processed 100000/220469 rows, matched 72921 flights with destination weather data\n",
      "Processed 110000/220469 rows, matched 80009 flights with destination weather data\n",
      "Processed 120000/220469 rows, matched 86941 flights with destination weather data\n",
      "Processed 130000/220469 rows, matched 93846 flights with destination weather data\n",
      "Processed 140000/220469 rows, matched 101149 flights with destination weather data\n",
      "Processed 150000/220469 rows, matched 108648 flights with destination weather data\n",
      "Processed 160000/220469 rows, matched 115811 flights with destination weather data\n",
      "Processed 170000/220469 rows, matched 123148 flights with destination weather data\n",
      "Processed 180000/220469 rows, matched 130462 flights with destination weather data\n",
      "Processed 190000/220469 rows, matched 137791 flights with destination weather data\n",
      "Processed 200000/220469 rows, matched 145224 flights with destination weather data\n",
      "Processed 210000/220469 rows, matched 152375 flights with destination weather data\n",
      "Processed 220000/220469 rows, matched 159902 flights with destination weather data\n",
      "Processed 220469/220469 rows, matched 160281 flights with destination weather data\n",
      "Matched destination weather data for 160281 flights (72.70%)\n",
      "Destination weather matching took: 79.90 seconds\n",
      "Selecting features...\n",
      "Using categorical features: ['YEAR', 'WEEK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'EXTREME_WEATHER', 'DEST_EXTREME_WEATHER']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'DEST_PRCP']\n",
      "Training set size: (176375, 14)\n",
      "Test set size: (44094, 14)\n",
      "Training Random Forest model...\n",
      "Model training took: 6.07 seconds\n",
      "Evaluating model...\n",
      "Accuracy: 99.41%\n",
      "ROC AUC: 0.7409\n",
      "\n",
      "Top 10 most important features:\n",
      "                 Feature  Importance\n",
      "1              num__PRCP    0.171666\n",
      "2         num__DEST_PRCP    0.096242\n",
      "0          num__DISTANCE    0.062436\n",
      "9            cat__WEEK_5    0.039584\n",
      "4            cat__WEEK_0    0.033694\n",
      "33  cat__ORIGIN_IATA_IAH    0.033204\n",
      "18   cat__MKT_AIRLINE_UA    0.032827\n",
      "63    cat__DEST_IATA_IAH    0.021701\n",
      "83     cat__IS_WEEKEND_1    0.020115\n",
      "28  cat__ORIGIN_IATA_DFW    0.019980\n",
      "\n",
      "Importance of indicator variables:\n",
      "cat__IS_REDEYE_1 importance: 0.006844 (rank: 82 out of 92)\n",
      "cat__IS_REDEYE_0 importance: 0.006106 (rank: 81 out of 92)\n",
      "cat__IS_WEEKEND_1 importance: 0.020115 (rank: 84 out of 92)\n",
      "cat__IS_WEEKEND_0 importance: 0.018849 (rank: 83 out of 92)\n",
      "cat__IS_MORNING_PEAK_1 importance: 0.014987 (rank: 86 out of 92)\n",
      "cat__IS_MORNING_PEAK_0 importance: 0.013584 (rank: 85 out of 92)\n",
      "cat__IS_EVENING_PEAK_1 importance: 0.015342 (rank: 88 out of 92)\n",
      "cat__IS_EVENING_PEAK_0 importance: 0.012902 (rank: 87 out of 92)\n",
      "\n",
      "Importance of weather features:\n",
      "num__PRCP importance: 0.171666 (rank: 2 out of 92)\n",
      "num__DEST_PRCP importance: 0.096242 (rank: 3 out of 92)\n",
      "cat__EXTREME_WEATHER_0.0 importance: 0.016840 (rank: 89 out of 92)\n",
      "cat__EXTREME_WEATHER_1.0 importance: 0.016537 (rank: 90 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_1.0 importance: 0.014350 (rank: 92 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_0.0 importance: 0.011670 (rank: 91 out of 92)\n",
      "Average effect of IS_REDEYE=1 on cancellation probability: 0.0026\n",
      "Average effect of IS_WEEKEND=1 on cancellation probability: 0.0027\n",
      "Average effect of IS_MORNING_PEAK=1 on cancellation probability: -0.0007\n",
      "Average effect of IS_EVENING_PEAK=1 on cancellation probability: 0.0017\n",
      "Model saved to ./cancelled_prob_rf_models/May2023_model.joblib\n",
      "Processing of May2023.csv completed in 172.20 seconds\n",
      "\n",
      "Processing file 4/4: May2024.csv (May 2024)\n",
      "Months found in data: {5: 649428}\n",
      "Filtered to only May data: 649428 rows\n",
      "Filtered from 649428 to 228159 rows for top 30 airports\n",
      "Standardizing day of week...\n",
      "Day of week range: 0 to 6 (0=Sunday, 1=Monday, ..., 6=Saturday)\n",
      "Creating red-eye flight indicator...\n",
      "Identified 14238 red-eye flights out of 228159 total flights (6.24%)\n",
      "Creating additional indicators: IS_WEEKEND, IS_MORNING_PEAK, IS_EVENING_PEAK...\n",
      "Identified 55434 weekend flights (24.30%)\n",
      "Identified 47154 morning peak flights (20.67%)\n",
      "Identified 41773 evening peak flights (18.31%)\n",
      "Final dataset shape: (228159, 54)\n",
      "Preprocessing data...\n",
      "Overall cancellation rate: 2994/228159 (1.31%)\n",
      "Red-eye flights cancellation rate: 1.76%\n",
      "Non-red-eye flights cancellation rate: 1.28%\n",
      "Weekend flights cancellation rate: 0.63%\n",
      "Weekday flights cancellation rate: 1.53%\n",
      "Morning peak flights cancellation rate: 0.90%\n",
      "Evening peak flights cancellation rate: 1.59%\n",
      "Off-peak flights cancellation rate: 1.37%\n",
      "Matching weather data with flights...\n",
      "\n",
      "Matching origin weather data with flights...\n",
      "Processed 10000/228159 rows, matched 7108 flights with origin weather data\n",
      "Processed 20000/228159 rows, matched 14424 flights with origin weather data\n",
      "Processed 30000/228159 rows, matched 21888 flights with origin weather data\n",
      "Processed 40000/228159 rows, matched 29168 flights with origin weather data\n",
      "Processed 50000/228159 rows, matched 36495 flights with origin weather data\n",
      "Processed 60000/228159 rows, matched 43822 flights with origin weather data\n",
      "Processed 70000/228159 rows, matched 51102 flights with origin weather data\n",
      "Processed 80000/228159 rows, matched 58545 flights with origin weather data\n",
      "Processed 90000/228159 rows, matched 65812 flights with origin weather data\n",
      "Processed 100000/228159 rows, matched 73043 flights with origin weather data\n",
      "Processed 110000/228159 rows, matched 80524 flights with origin weather data\n",
      "Processed 120000/228159 rows, matched 87743 flights with origin weather data\n",
      "Processed 130000/228159 rows, matched 95045 flights with origin weather data\n",
      "Processed 140000/228159 rows, matched 102536 flights with origin weather data\n",
      "Processed 150000/228159 rows, matched 109677 flights with origin weather data\n",
      "Processed 160000/228159 rows, matched 116990 flights with origin weather data\n",
      "Processed 170000/228159 rows, matched 124459 flights with origin weather data\n",
      "Processed 180000/228159 rows, matched 131653 flights with origin weather data\n",
      "Processed 190000/228159 rows, matched 139083 flights with origin weather data\n",
      "Processed 200000/228159 rows, matched 146309 flights with origin weather data\n",
      "Processed 210000/228159 rows, matched 153551 flights with origin weather data\n",
      "Processed 220000/228159 rows, matched 161015 flights with origin weather data\n",
      "Processed 228159/228159 rows, matched 167051 flights with origin weather data\n",
      "Matched origin weather data for 167051 flights (73.22%)\n",
      "Origin weather matching took: 79.34 seconds\n",
      "\n",
      "Matching destination weather data with flights...\n",
      "Processed 10000/228159 rows, matched 7119 flights with destination weather data\n",
      "Processed 20000/228159 rows, matched 14420 flights with destination weather data\n",
      "Processed 30000/228159 rows, matched 21907 flights with destination weather data\n",
      "Processed 40000/228159 rows, matched 29170 flights with destination weather data\n",
      "Processed 50000/228159 rows, matched 36480 flights with destination weather data\n",
      "Processed 60000/228159 rows, matched 43806 flights with destination weather data\n",
      "Processed 70000/228159 rows, matched 51084 flights with destination weather data\n",
      "Processed 80000/228159 rows, matched 58538 flights with destination weather data\n",
      "Processed 90000/228159 rows, matched 65799 flights with destination weather data\n",
      "Processed 100000/228159 rows, matched 73033 flights with destination weather data\n",
      "Processed 110000/228159 rows, matched 80513 flights with destination weather data\n",
      "Processed 120000/228159 rows, matched 87723 flights with destination weather data\n",
      "Processed 130000/228159 rows, matched 95030 flights with destination weather data\n",
      "Processed 140000/228159 rows, matched 102525 flights with destination weather data\n",
      "Processed 150000/228159 rows, matched 109663 flights with destination weather data\n",
      "Processed 160000/228159 rows, matched 116970 flights with destination weather data\n",
      "Processed 170000/228159 rows, matched 124460 flights with destination weather data\n",
      "Processed 180000/228159 rows, matched 131648 flights with destination weather data\n",
      "Processed 190000/228159 rows, matched 139060 flights with destination weather data\n",
      "Processed 200000/228159 rows, matched 146293 flights with destination weather data\n",
      "Processed 210000/228159 rows, matched 153534 flights with destination weather data\n",
      "Processed 220000/228159 rows, matched 160998 flights with destination weather data\n",
      "Processed 228159/228159 rows, matched 167035 flights with destination weather data\n",
      "Matched destination weather data for 167035 flights (73.21%)\n",
      "Destination weather matching took: 80.75 seconds\n",
      "Selecting features...\n",
      "Using categorical features: ['YEAR', 'WEEK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'EXTREME_WEATHER', 'DEST_EXTREME_WEATHER']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'DEST_PRCP']\n",
      "Training set size: (182527, 14)\n",
      "Test set size: (45632, 14)\n",
      "Training Random Forest model...\n",
      "Model training took: 6.25 seconds\n",
      "Evaluating model...\n",
      "Accuracy: 98.69%\n",
      "ROC AUC: 0.8350\n",
      "\n",
      "Top 10 most important features:\n",
      "                 Feature  Importance\n",
      "58    cat__DEST_IATA_DFW    0.147709\n",
      "28  cat__ORIGIN_IATA_DFW    0.141844\n",
      "1              num__PRCP    0.110620\n",
      "11   cat__MKT_AIRLINE_AA    0.090123\n",
      "2         num__DEST_PRCP    0.074959\n",
      "14   cat__MKT_AIRLINE_DL    0.058446\n",
      "0          num__DISTANCE    0.051585\n",
      "8            cat__WEEK_4    0.030129\n",
      "83     cat__IS_WEEKEND_1    0.023865\n",
      "82     cat__IS_WEEKEND_0    0.022534\n",
      "\n",
      "Importance of indicator variables:\n",
      "cat__IS_REDEYE_1 importance: 0.003190 (rank: 82 out of 92)\n",
      "cat__IS_REDEYE_0 importance: 0.002596 (rank: 81 out of 92)\n",
      "cat__IS_WEEKEND_1 importance: 0.023865 (rank: 84 out of 92)\n",
      "cat__IS_WEEKEND_0 importance: 0.022534 (rank: 83 out of 92)\n",
      "cat__IS_MORNING_PEAK_1 importance: 0.008847 (rank: 86 out of 92)\n",
      "cat__IS_MORNING_PEAK_0 importance: 0.008705 (rank: 85 out of 92)\n",
      "cat__IS_EVENING_PEAK_0 importance: 0.003907 (rank: 87 out of 92)\n",
      "cat__IS_EVENING_PEAK_1 importance: 0.003765 (rank: 88 out of 92)\n",
      "\n",
      "Importance of weather features:\n",
      "num__PRCP importance: 0.110620 (rank: 2 out of 92)\n",
      "num__DEST_PRCP importance: 0.074959 (rank: 3 out of 92)\n",
      "cat__EXTREME_WEATHER_0.0 importance: 0.006824 (rank: 89 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_1.0 importance: 0.006306 (rank: 92 out of 92)\n",
      "cat__EXTREME_WEATHER_1.0 importance: 0.006057 (rank: 90 out of 92)\n",
      "cat__DEST_EXTREME_WEATHER_0.0 importance: 0.005698 (rank: 91 out of 92)\n",
      "Average effect of IS_REDEYE=1 on cancellation probability: 0.0034\n",
      "Average effect of IS_WEEKEND=1 on cancellation probability: -0.0207\n",
      "Average effect of IS_MORNING_PEAK=1 on cancellation probability: -0.0080\n",
      "Average effect of IS_EVENING_PEAK=1 on cancellation probability: 0.0015\n",
      "Model saved to ./cancelled_prob_rf_models/May2024_model.joblib\n",
      "Processing of May2024.csv completed in 173.95 seconds\n",
      "\n",
      "Summary of Random Forest model training:\n",
      "Successfully trained models: 4/4\n",
      "Failed models: 0/4\n",
      "Skipped files: 0/4\n",
      "\n",
      "Average metrics across all successful models:\n",
      "Accuracy: 98.90%\n",
      "ROC AUC: 0.8050\n",
      "Precision: 0.2500\n",
      "Recall: 0.0003\n",
      "Origin weather data match rate: 100.00%\n",
      "Destination weather data match rate: 100.00%\n",
      "\n",
      "Most common important features across all models:\n",
      "num__DISTANCE: Appears in 3 models (75.0%)\n",
      "num__DEST_PRCP: Appears in 3 models (75.0%)\n",
      "cat__ORIGIN_IATA_DFW: Appears in 2 models (50.0%)\n",
      "cat__DEST_IATA_DFW: Appears in 2 models (50.0%)\n",
      "cat__MKT_AIRLINE_AA: Appears in 2 models (50.0%)\n",
      "cat__WEEK_5: Appears in 2 models (50.0%)\n",
      "num__PRCP: Appears in 2 models (50.0%)\n",
      "cat__WEEK_2: Appears in 1 models (25.0%)\n",
      "cat__MKT_AIRLINE_DL: Appears in 1 models (25.0%)\n",
      "cat__WEEK_3: Appears in 1 models (25.0%)\n",
      "\n",
      "IS_REDEYE generally increases cancellation probability by 0.0014\n",
      "Average IS_REDEYE importance: 0.003175\n",
      "\n",
      "IS_WEEKEND generally increases cancellation probability by 0.0011\n",
      "Average IS_WEEKEND importance: 0.020912\n",
      "\n",
      "IS_MORNING_PEAK generally decreases cancellation probability by 0.0036\n",
      "Average IS_MORNING_PEAK importance: 0.011768\n",
      "\n",
      "IS_EVENING_PEAK generally increases cancellation probability by 0.0035\n",
      "Average IS_EVENING_PEAK importance: 0.010782\n",
      "\n",
      "Average indicator effects comparison saved to average_all_indicators_effect.png\n",
      "\n",
      "Random Forest model training with enhanced indicators and destination weather complete!\n",
      "Full summary saved to ./cancelled_prob_rf_models/cancelled_prob_rf_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
