{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:08:51.453492Z",
     "start_time": "2025-04-17T22:08:51.428495Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from joblib import dump\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.DataFrame,)):\n",
    "        return obj.to_dict('records')\n",
    "    elif isinstance(obj, (pd.Series,)):\n",
    "        return obj.to_dict()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Set data paths\n",
    "flight_data_path = './cleaned_data/'\n",
    "weather_data_path = './cleaned_weather_data/'\n",
    "top_airports_file = './top_100_airports.csv'\n",
    "output_dir = './dep_delay_lr/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting year-by-year flight delay prediction models (Logistic Regression)...\")\n",
    "print(f\"Flight data directory: {flight_data_path}\")\n",
    "print(f\"Weather data directory: {weather_data_path}\")\n",
    "print(f\"Top airports file: {top_airports_file}\")\n",
    "print(f\"Model output directory: {output_dir}\")\n",
    "\n",
    "# Load top 30 airports from the top 100 airports file\n",
    "try:\n",
    "    top_airports = pd.read_csv(top_airports_file, low_memory=False)\n",
    "    \n",
    "    top_airports = top_airports.head(30)\n",
    "    \n",
    "    top_airport_codes = set(top_airports['ORIGIN_IATA'].str.strip().tolist())\n",
    "    \n",
    "    print(f\"Loaded top 30 airports: {', '.join(sorted(top_airport_codes))}\")\n",
    "    print(f\"Busiest airport: {top_airports.iloc[0]['ORIGIN_IATA']} with {top_airports.iloc[0]['Times']} flights\")\n",
    "    print(f\"30th busiest airport: {top_airports.iloc[29]['ORIGIN_IATA']} with {top_airports.iloc[29]['Times']} flights\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading top airports file: {e}\")\n",
    "    top_airport_codes = None\n",
    "    print(\"Will process all airports (top airports file not available)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting year-by-year flight delay prediction models (Logistic Regression)...\n",
      "Flight data directory: ./cleaned_data/\n",
      "Weather data directory: ./cleaned_weather_data/\n",
      "Top airports file: ./top_100_airports.csv\n",
      "Model output directory: ./dep_delay_lr/\n",
      "Loaded top 30 airports: ATL, AUS, BNA, BOS, BWI, CLT, DCA, DEN, DFW, DTW, EWR, FLL, IAD, IAH, JFK, LAS, LAX, LGA, MCO, MDW, MIA, MSP, ORD, PHL, PHX, SAN, SEA, SFO, SLC, TPA\n",
      "Busiest airport: ATL with 457121 flights\n",
      "30th busiest airport: TPA with 97235 flights\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:08:53.577537Z",
     "start_time": "2025-04-17T22:08:51.473020Z"
    }
   },
   "source": [
    "# Function to load weather data\n",
    "def load_weather_data():\n",
    "    print(\"\\nLoading weather data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_files = glob.glob(os.path.join(weather_data_path, \"*.csv\"))\n",
    "    print(f\"Found {len(all_files)} total weather data files\")\n",
    "    weather_dict = {}\n",
    "    count = 0\n",
    "    matching_count = 0\n",
    "\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            filename = os.path.basename(file)\n",
    "            parts = filename.split('.')[0].split('_')\n",
    "            \n",
    "            if len(parts) >= 3:\n",
    "                iata = parts[0]\n",
    "                year = parts[1]\n",
    "                month_name = parts[2]\n",
    "                \n",
    "                # Convert month name to number\n",
    "                month_map = {\n",
    "                    'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "                    'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "                    'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "                }\n",
    "                \n",
    "                if month_name in month_map:\n",
    "                    month = month_map[month_name]\n",
    "                    \n",
    "                    if top_airport_codes is None or iata in top_airport_codes:\n",
    "                        weather_data = pd.read_csv(file, low_memory=False)\n",
    "\n",
    "                        if 'DATE' not in weather_data.columns:\n",
    "                            print(f\"Warning: DATE column not found in {filename}\")\n",
    "                            continue\n",
    "\n",
    "                        weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "\n",
    "                        key = f\"{iata}_{year}_{month}\"\n",
    "\n",
    "                        weather_dict[key] = weather_data\n",
    "                        matching_count += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Unknown month format in {filename}\")\n",
    "                \n",
    "                count += 1\n",
    "\n",
    "                if count % 100 == 0:\n",
    "                    print(f\"Processed {count} weather files, loaded {matching_count} matching files\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading weather file {file}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {matching_count} weather files out of {count} processed files\")\n",
    "    print(f\"Loading weather data took: {time.time() - start_time:.2f} seconds\")\n",
    "    return weather_dict\n",
    "\n",
    "def get_may_files():\n",
    "    may_files = [\n",
    "        os.path.join(flight_data_path, \"May2021.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2022.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2023.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2024.csv\")\n",
    "    ]\n",
    "\n",
    "    existing_files = []\n",
    "    for file_path in may_files:\n",
    "        if os.path.exists(file_path):\n",
    "            existing_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} not found\")\n",
    "    \n",
    "    return existing_files\n",
    "\n",
    "flight_files = get_may_files()\n",
    "print(f\"\\nFound {len(flight_files)} May files to process:\")\n",
    "for f in flight_files:\n",
    "    print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "if not flight_files:\n",
    "    print(\"No May 2021-2024 files were found. Please check file paths.\")\n",
    "    exit(1)\n",
    "\n",
    "# Load all weather data once\n",
    "weather_dict = load_weather_data()\n",
    "\n",
    "# Function to extract year from filename\n",
    "def extract_year_from_filename(filename):\n",
    "    base_name = os.path.basename(filename)\n",
    "    year_str = base_name.replace('May', '').split('.')[0]\n",
    "    return int(year_str)\n",
    "\n",
    "# Function to create red-eye flight indicator\n",
    "def create_redeye_indicator(df):\n",
    "    # Make a copy\n",
    "    df = df.copy()\n",
    "\n",
    "    df['IS_REDEYE'] = 0\n",
    "    time_columns = []\n",
    "\n",
    "    if 'SCH_DEP_TIME' in df.columns:\n",
    "        time_columns.append('SCH_DEP_TIME')\n",
    "\n",
    "    if 'SCH_ARR_TIME' in df.columns:\n",
    "        time_columns.append('SCH_ARR_TIME')\n",
    "\n",
    "    for col in time_columns:\n",
    "        if df[col].dtype != 'float64':\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert {col} to numeric\")\n",
    "    \n",
    "    # Identify red-eye flights\n",
    "    if 'SCH_DEP_TIME' in time_columns:\n",
    "        redeye_departure = (df['SCH_DEP_TIME'] >= 0) & (df['SCH_DEP_TIME'] < 600)\n",
    "        df.loc[redeye_departure, 'IS_REDEYE'] = 1\n",
    "        \n",
    "        # Count departures identified as red-eye\n",
    "        dep_redeye_count = redeye_departure.sum()\n",
    "        print(f\"Identified {dep_redeye_count} red-eye flights based on departure time (0-6 AM)\")\n",
    "\n",
    "    if 'SCH_ARR_TIME' in time_columns:\n",
    "        redeye_arrival = (df['SCH_ARR_TIME'] >= 0) & (df['SCH_ARR_TIME'] < 600)\n",
    "        df.loc[redeye_arrival, 'IS_REDEYE'] = 1\n",
    "        \n",
    "        # Count arrivals identified as red-eye\n",
    "        arr_redeye_count = redeye_arrival.sum()\n",
    "        print(f\"Identified {arr_redeye_count} red-eye flights based on arrival time (0-6 AM)\")\n",
    "\n",
    "    redeye_count = df['IS_REDEYE'].sum()\n",
    "    total_count = len(df)\n",
    "    print(f\"Total identified red-eye flights: {redeye_count} out of {total_count} total flights ({redeye_count/total_count*100:.2f}%)\")\n",
    "\n",
    "    if 'SCH_DEP_TIME' in time_columns:\n",
    "        df['DEP_TIME_OF_DAY'] = pd.cut(\n",
    "            df['SCH_DEP_TIME'], \n",
    "            bins=[0, 600, 1200, 1800, 2400],\n",
    "            labels=['Early Morning (0-6)', 'Morning (6-12)', 'Afternoon (12-18)', 'Evening (18-24)'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "\n",
    "        time_dist = df['DEP_TIME_OF_DAY'].value_counts()\n",
    "        print(\"\\nDistribution of flights by departure time of day:\")\n",
    "        for time_cat, count in time_dist.items():\n",
    "            print(f\"  - {time_cat}: {count} flights ({count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to prepare departure delay data\n",
    "def prepare_delay_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'DEP_DELAY' in df.columns:\n",
    "        if df['DEP_DELAY'].dtype != 'float64':\n",
    "            try:\n",
    "                df['DEP_DELAY'] = pd.to_numeric(df['DEP_DELAY'], errors='coerce')\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert DEP_DELAY to numeric\")\n",
    "    else:\n",
    "        print(\"Warning: DEP_DELAY column not found in dataset\")\n",
    "        return df\n",
    "\n",
    "    df['IS_DELAYED'] = (df['DEP_DELAY'] > 0).astype(int)\n",
    "    \n",
    "    # Create a categorical delay feature\n",
    "    df['DELAY_CATEGORY'] = pd.cut(\n",
    "        df['DEP_DELAY'],\n",
    "        bins=[-float('inf'), -15, 0, 15, 60, 120, float('inf')],\n",
    "        labels=['Very Early', 'Early', 'On Time', 'Moderate Delay',\n",
    "                'Significant Delay', 'Severe Delay'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    df['ABS_DELAY'] = np.abs(df['DEP_DELAY'])\n",
    "    \n",
    "    # Print delay statistics\n",
    "    delay_count = df['IS_DELAYED'].sum()\n",
    "    total_count = len(df)\n",
    "    delay_rate = delay_count / total_count * 100\n",
    "    \n",
    "    print(f\"\\nDelay statistics:\")\n",
    "    print(f\"Delayed flights: {delay_count}/{total_count} ({delay_rate:.2f}%)\")\n",
    "    print(f\"On-time or early flights: {total_count - delay_count}/{total_count} ({100 - delay_rate:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nDelay magnitude statistics:\")\n",
    "    print(f\"Mean delay: {df['DEP_DELAY'].mean():.2f} minutes\")\n",
    "    print(f\"Median delay: {df['DEP_DELAY'].median():.2f} minutes\")\n",
    "    print(f\"Min delay: {df['DEP_DELAY'].min():.2f} minutes (negative means early departure)\")\n",
    "    print(f\"Max delay: {df['DEP_DELAY'].max():.2f} minutes\")\n",
    "\n",
    "    delay_cat_dist = df['DELAY_CATEGORY'].value_counts()\n",
    "    print(\"\\nDelay category distribution:\")\n",
    "    for cat, count in delay_cat_dist.sort_index().items():\n",
    "        print(f\"  - {cat}: {count} flights ({count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create time block features\n",
    "def create_time_block_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'SCH_DEP_TIME' not in df.columns:\n",
    "        print(\"Warning: SCH_DEP_TIME column not found for time block features\")\n",
    "        return df\n",
    "\n",
    "    if df['SCH_DEP_TIME'].dtype != 'float64':\n",
    "        try:\n",
    "            df['SCH_DEP_TIME'] = pd.to_numeric(df['SCH_DEP_TIME'], errors='coerce')\n",
    "        except:\n",
    "            print(f\"Warning: Could not convert SCH_DEP_TIME to numeric\")\n",
    "            return df\n",
    "\n",
    "    df['DEP_HOUR'] = (df['SCH_DEP_TIME'] / 100).astype(int)\n",
    "\n",
    "    time_blocks = {\n",
    "        0: 'Late Night (0-3)',\n",
    "        1: 'Late Night (0-3)',\n",
    "        2: 'Late Night (0-3)',\n",
    "        3: 'Early Morning (3-6)',\n",
    "        4: 'Early Morning (3-6)',\n",
    "        5: 'Early Morning (3-6)',\n",
    "        6: 'Morning (6-9)',\n",
    "        7: 'Morning (6-9)',\n",
    "        8: 'Morning (6-9)',\n",
    "        9: 'Mid-Day (9-12)',\n",
    "        10: 'Mid-Day (9-12)',\n",
    "        11: 'Mid-Day (9-12)',\n",
    "        12: 'Afternoon (12-15)',\n",
    "        13: 'Afternoon (12-15)',\n",
    "        14: 'Afternoon (12-15)',\n",
    "        15: 'Evening (15-18)',\n",
    "        16: 'Evening (15-18)',\n",
    "        17: 'Evening (15-18)',\n",
    "        18: 'Night (18-21)',\n",
    "        19: 'Night (18-21)',\n",
    "        20: 'Night (18-21)',\n",
    "        21: 'Late Night (21-24)',\n",
    "        22: 'Late Night (21-24)',\n",
    "        23: 'Late Night (21-24)'\n",
    "    }\n",
    "\n",
    "    df['TIME_BLOCK'] = df['DEP_HOUR'].map(time_blocks)\n",
    "    \n",
    "    # Morning peak (7-9 AM)\n",
    "    df['IS_MORNING_PEAK'] = ((df['DEP_HOUR'] >= 7) & (df['DEP_HOUR'] <= 9)).astype(int)\n",
    "    \n",
    "    # Evening peak (4-7 PM)\n",
    "    df['IS_EVENING_PEAK'] = ((df['DEP_HOUR'] >= 16) & (df['DEP_HOUR'] <= 19)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create day of week features\n",
    "def create_day_features(df):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check if we have the WEEK column with text day names\n",
    "    if 'WEEK' in df.columns:\n",
    "        # Create a mapping from abbreviated day names to full day names\n",
    "        day_name_map = {\n",
    "            'Sun': 'Sunday',\n",
    "            'Mon': 'Monday',\n",
    "            'Tue': 'Tuesday',\n",
    "            'Wed': 'Wednesday',\n",
    "            'Thu': 'Thursday',\n",
    "            'Fri': 'Friday',\n",
    "            'Sat': 'Saturday'\n",
    "        }\n",
    "        \n",
    "        # Map abbreviated names to full names\n",
    "        df['DAY_NAME'] = df['WEEK'].map(day_name_map)\n",
    "        \n",
    "        # Create weekend indicator\n",
    "        df['IS_WEEKEND'] = df['WEEK'].isin(['Sat', 'Sun']).astype(int)\n",
    "\n",
    "        day_counts = df['DAY_NAME'].value_counts()\n",
    "        total = len(df)\n",
    "        print(\"\\nDistribution of flights by day of week:\")\n",
    "        for day, count in day_counts.items():\n",
    "            print(f\"  - {day}: {count} flights ({count/total*100:.2f}%)\")\n",
    "\n",
    "        weekend_count = df['IS_WEEKEND'].sum()\n",
    "        weekday_count = total - weekend_count\n",
    "        print(f\"\\nWeekend flights: {weekend_count} ({weekend_count/total*100:.2f}%)\")\n",
    "        print(f\"Weekday flights: {weekday_count} ({weekday_count/total*100:.2f}%)\")\n",
    "        \n",
    "    elif 'DAY_OF_WEEK' in df.columns:\n",
    "        max_day = df['DAY_OF_WEEK'].max()\n",
    "        \n",
    "        if max_day == 7:\n",
    "            df['IS_WEEKEND'] = ((df['DAY_OF_WEEK'] == 6) | (df['DAY_OF_WEEK'] == 7)).astype(int)\n",
    "\n",
    "            day_names = {1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', \n",
    "                        4: 'Thursday', 5: 'Friday', 6: 'Saturday', 7: 'Sunday'}\n",
    "        else:\n",
    "            df['IS_WEEKEND'] = ((df['DAY_OF_WEEK'] == 5) | (df['DAY_OF_WEEK'] == 6)).astype(int)\n",
    "\n",
    "            day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n",
    "                        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "        \n",
    "        df['DAY_NAME'] = df['DAY_OF_WEEK'].map(day_names)\n",
    "    else:\n",
    "        print(\"Warning: No day of week column (WEEK or DAY_OF_WEEK) found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to load and preprocess a single flight data file\n",
    "def load_and_process_flight_data(file_path):\n",
    "    print(f\"\\nProcessing {os.path.basename(file_path)}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        original_size = len(df)\n",
    "\n",
    "        file_year = extract_year_from_filename(file_path)\n",
    "        \n",
    "        if 'YEAR' in df.columns:\n",
    "            unique_years = df['YEAR'].unique()\n",
    "            print(f\"Years found in data: {unique_years}\")\n",
    "\n",
    "            if len(unique_years) > 1:\n",
    "                df = df[df['YEAR'] == file_year]\n",
    "                print(f\"Filtered to only year {file_year}: {len(df)} rows\")\n",
    "        else:\n",
    "            df['YEAR'] = file_year\n",
    "            print(f\"Added YEAR column with value {file_year}\")\n",
    "\n",
    "        if 'MONTH' in df.columns:\n",
    "            month_counts = df['MONTH'].value_counts()\n",
    "            print(f\"Months found in data: {dict(month_counts)}\")\n",
    "            \n",
    "            if 5 in month_counts:\n",
    "                df = df[df['MONTH'] == 5]\n",
    "                print(f\"Filtered to only May data: {len(df)} rows\")\n",
    "            else:\n",
    "                print(f\"Warning: No May data found in file, but proceeding anyway as this should be May data based on filename\")\n",
    "\n",
    "        if 'DEP_DELAY' not in df.columns:\n",
    "            print(f\"DEP_DELAY column not found in {os.path.basename(file_path)}. Skipping file.\")\n",
    "            return None\n",
    "\n",
    "        if top_airport_codes is not None:\n",
    "            df = df[\n",
    "                df['ORIGIN_IATA'].str.strip().isin(top_airport_codes) & \n",
    "                df['DEST_IATA'].str.strip().isin(top_airport_codes)\n",
    "            ]\n",
    "            \n",
    "            filtered_size = len(df)\n",
    "            print(f\"Filtered from {original_size} to {filtered_size} rows for top 30 airports\")\n",
    "\n",
    "            if filtered_size == 0:\n",
    "                print(f\"No data remaining after filtering for top 30 airports. Skipping file.\")\n",
    "                return None\n",
    "\n",
    "        if 'CANCELLED' in df.columns:\n",
    "            cancelled_count = df['CANCELLED'].sum()\n",
    "            if cancelled_count > 0:\n",
    "                df = df[df['CANCELLED'] == 0]\n",
    "                print(f\"Removed {cancelled_count} cancelled flights, remaining: {len(df)}\")\n",
    "        \n",
    "        print(f\"Processing took: {time.time() - start_time:.2f} seconds\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to match weather data to flights\n",
    "def match_weather_data(df):\n",
    "    print(\"\\nMatching weather data with flights...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    date_columns_exist = all(col in df.columns for col in ['YEAR', 'MONTH', 'DAY'])\n",
    "    if not date_columns_exist:\n",
    "        print(\"Warning: Missing one or more date columns (YEAR, MONTH, DAY)\")\n",
    "        print(\"Weather data cannot be matched\")\n",
    "        return df\n",
    "\n",
    "    df['FLIGHT_DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']])\n",
    "\n",
    "    df['WEATHER_KEY'] = df['ORIGIN_IATA'] + '_' + df['YEAR'].astype(str) + '_' + df['MONTH'].astype(str).str.zfill(2)\n",
    "\n",
    "    weather_columns = ['EXTREME_WEATHER', 'PRCP', 'WT01', 'WT03', 'WT04', 'WT05', 'WT08', 'WT11']\n",
    "    for col in weather_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    \n",
    "    # Process in batches\n",
    "    matched_count = 0\n",
    "    batch_size = 10000\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                weather_key = row['WEATHER_KEY']\n",
    "                flight_date = row['FLIGHT_DATE']\n",
    "\n",
    "                if weather_key in weather_dict:\n",
    "                    weather_data = weather_dict[weather_key]\n",
    "\n",
    "                    matching_weather = weather_data[weather_data['DATE'] == flight_date]\n",
    "                    \n",
    "                    if not matching_weather.empty:\n",
    "                        for col in weather_columns:\n",
    "                            if col in matching_weather.columns:\n",
    "                                df.at[idx, col] = matching_weather[col].iloc[0]\n",
    "                        matched_count += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Processed {end_idx}/{len(df)} rows, matched {matched_count} flights with weather data\")\n",
    "    \n",
    "    print(f\"Matched weather data for {matched_count} flights ({matched_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"Weather matching took: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to visualize coefficients (for logistic regression)\n",
    "def plot_model_coefficients(model, feature_names, year, top_n=15, output_path=None):\n",
    "    coefficients = model.coef_[0]\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "\n",
    "    coef_df['AbsCoef'] = np.abs(coef_df['Coefficient'])\n",
    "    coef_df = coef_df.sort_values('AbsCoef', ascending=False)\n",
    "    \n",
    "    # Take top features\n",
    "    top_coef = coef_df.head(top_n)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    colors = ['red' if c < 0 else 'blue' for c in top_coef['Coefficient']]\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=top_coef, palette=colors)\n",
    "    \n",
    "    plt.title(f'Top {top_n} Most Influential Features (Logistic Regression - {year})')\n",
    "    plt.axvline(x=0, color='gray', linestyle='--')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Coefficient plot saved to {output_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return coef_df"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4 May files to process:\n",
      "  - May2021.csv\n",
      "  - May2022.csv\n",
      "  - May2023.csv\n",
      "  - May2024.csv\n",
      "\n",
      "Loading weather data...\n",
      "Found 3550 total weather data files\n",
      "Processed 100 weather files, loaded 0 matching files\n",
      "Processed 200 weather files, loaded 0 matching files\n",
      "Processed 300 weather files, loaded 16 matching files\n",
      "Processed 400 weather files, loaded 32 matching files\n",
      "Processed 500 weather files, loaded 32 matching files\n",
      "Processed 600 weather files, loaded 48 matching files\n",
      "Processed 700 weather files, loaded 48 matching files\n",
      "Processed 800 weather files, loaded 48 matching files\n",
      "Processed 900 weather files, loaded 64 matching files\n",
      "Processed 1000 weather files, loaded 64 matching files\n",
      "Processed 1100 weather files, loaded 80 matching files\n",
      "Processed 1200 weather files, loaded 96 matching files\n",
      "Processed 1300 weather files, loaded 112 matching files\n",
      "Processed 1400 weather files, loaded 112 matching files\n",
      "Processed 1500 weather files, loaded 112 matching files\n",
      "Processed 1600 weather files, loaded 112 matching files\n",
      "Processed 1700 weather files, loaded 128 matching files\n",
      "Processed 1800 weather files, loaded 128 matching files\n",
      "Processed 1900 weather files, loaded 143 matching files\n",
      "Processed 2000 weather files, loaded 160 matching files\n",
      "Processed 2100 weather files, loaded 160 matching files\n",
      "Processed 2200 weather files, loaded 192 matching files\n",
      "Processed 2300 weather files, loaded 208 matching files\n",
      "Processed 2400 weather files, loaded 208 matching files\n",
      "Processed 2500 weather files, loaded 224 matching files\n",
      "Processed 2600 weather files, loaded 240 matching files\n",
      "Processed 2700 weather files, loaded 240 matching files\n",
      "Processed 2800 weather files, loaded 256 matching files\n",
      "Processed 2900 weather files, loaded 256 matching files\n",
      "Processed 3000 weather files, loaded 272 matching files\n",
      "Processed 3100 weather files, loaded 286 matching files\n",
      "Processed 3200 weather files, loaded 306 matching files\n",
      "Processed 3300 weather files, loaded 320 matching files\n",
      "Processed 3400 weather files, loaded 336 matching files\n",
      "Processed 3500 weather files, loaded 336 matching files\n",
      "Loaded 336 weather files out of 3550 processed files\n",
      "Loading weather data took: 2.01 seconds\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:08:54.478738Z",
     "start_time": "2025-04-17T22:08:53.615076Z"
    }
   },
   "source": [
    "# Function to train model\n",
    "def train_year_model(year, flight_data_file):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training model for year {year}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    year_output_dir = os.path.join(output_dir, f'year_{year}')\n",
    "    os.makedirs(year_output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(year_output_dir, 'metrics'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(year_output_dir, 'plots'), exist_ok=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load and preprocess the year's flight data\n",
    "    flight_data = load_and_process_flight_data(flight_data_file)\n",
    "    if flight_data is None or len(flight_data) == 0:\n",
    "        print(f\"No valid flight data available for {year}. Skipping this year.\")\n",
    "        return None\n",
    "\n",
    "    flight_data = match_weather_data(flight_data)\n",
    "    \n",
    "    # Add red-eye flight indicator\n",
    "    print(f\"\\nCreating red-eye flight indicator for {year}...\")\n",
    "    flight_data = create_redeye_indicator(flight_data)\n",
    "    \n",
    "    # Prepare delay data\n",
    "    print(f\"\\nPreparing delay data for {year}...\")\n",
    "    flight_data = prepare_delay_data(flight_data)\n",
    "    \n",
    "    # Create time block features\n",
    "    print(f\"\\nCreating time block features for {year}...\")\n",
    "    flight_data = create_time_block_features(flight_data)\n",
    "    \n",
    "    # Create day features - now handling text day names\n",
    "    print(f\"\\nCreating day features for {year}...\")\n",
    "    flight_data = create_day_features(flight_data)\n",
    "    \n",
    "    # Feature selection\n",
    "    print(f\"\\nSelecting features for delay prediction for {year}...\")\n",
    "    \n",
    "    # Categorical features\n",
    "    cat_features = ['DAY_NAME', 'TIME_BLOCK',\n",
    "                    'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'EXTREME_WEATHER',\n",
    "                    'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']\n",
    "    \n",
    "    # Numerical features\n",
    "    num_features = ['DISTANCE', 'PRCP']\n",
    "    \n",
    "    cat_features = [f for f in cat_features if f in flight_data.columns]\n",
    "    num_features = [f for f in num_features if f in flight_data.columns]\n",
    "    \n",
    "    print(f\"Using categorical features: {cat_features}\")\n",
    "    print(f\"Using numerical features: {num_features}\")\n",
    "    \n",
    "    # Prepare data for modeling\n",
    "    X = flight_data[cat_features + num_features].copy()\n",
    "    y_class = flight_data['IS_DELAYED']\n",
    "    y_reg = flight_data['DEP_DELAY']\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in cat_features:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna('unknown', inplace=True)\n",
    "    for col in num_features:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    # Split data for classification model\n",
    "    X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "        X, y_class, test_size=0.1, random_state=2025, stratify=y_class\n",
    "    )\n",
    "    \n",
    "    # Split data for regression model\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "        X, y_reg, test_size=0.1, random_state=2025\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {X_train_class.shape}\")\n",
    "    print(f\"Test set size: {X_test_class.shape}\")\n",
    "    \n",
    "    # Define preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, num_features),\n",
    "            ('cat', categorical_transformer, cat_features)\n",
    "        ])\n",
    "    \n",
    "    # Train classification model\n",
    "    print(f\"\\nTraining delay classification model for {year} (Logistic Regression)...\")\n",
    "    class_model_start_time = time.time()\n",
    "\n",
    "    class_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            C=0.1,\n",
    "            class_weight='balanced',\n",
    "            random_state=2025,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train classification model\n",
    "    class_model.fit(X_train_class, y_train_class)\n",
    "    final_class_model = class_model\n",
    "    \n",
    "    class_model_training_time = time.time() - class_model_start_time\n",
    "    print(f\"Classification model training took: {class_model_training_time:.2f} seconds\")\n",
    "    \n",
    "    # Train regression model (Linear Regression)\n",
    "    print(f\"\\nTraining delay regression model for {year} (Linear Regression)...\")\n",
    "    reg_model_start_time = time.time()\n",
    "    \n",
    "    reg_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression(\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train regression model\n",
    "    reg_model.fit(X_train_reg, y_train_reg)\n",
    "    final_reg_model = reg_model\n",
    "    \n",
    "    reg_model_training_time = time.time() - reg_model_start_time\n",
    "    print(f\"Regression model training took: {reg_model_training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate classification model\n",
    "    print(f\"\\nEvaluating delay classification model for {year}...\")\n",
    "    \n",
    "    y_pred_class = final_class_model.predict(X_test_class)\n",
    "    y_prob_class = final_class_model.predict_proba(X_test_class)[:, 1]\n",
    "    \n",
    "    class_accuracy = (y_pred_class == y_test_class).mean() * 100\n",
    "    class_roc_auc = roc_auc_score(y_test_class, y_prob_class)\n",
    "\n",
    "    class_report = classification_report(y_test_class, y_pred_class, output_dict=True)\n",
    "\n",
    "    class_cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "    \n",
    "    print(f\"Classification Accuracy: {class_accuracy:.2f}%\")\n",
    "    print(f\"Classification ROC AUC: {class_roc_auc:.4f}\")\n",
    "    print(f\"Classification Precision (Delayed): {class_report['1']['precision']:.4f}\")\n",
    "    print(f\"Classification Recall (Delayed): {class_report['1']['recall']:.4f}\")\n",
    "    print(f\"Classification F1 Score (Delayed): {class_report['1']['f1-score']:.4f}\")\n",
    "    \n",
    "    #Evaluate regression model\n",
    "    print(f\"\\nEvaluating delay regression model for {year}...\")\n",
    "\n",
    "    y_pred_reg = final_reg_model.predict(X_test_reg)\n",
    "\n",
    "    reg_mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    reg_rmse = np.sqrt(reg_mse)\n",
    "    reg_mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "    reg_r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "    \n",
    "    print(f\"Regression Mean Squared Error: {reg_mse:.2f}\")\n",
    "    print(f\"Regression Root Mean Squared Error: {reg_rmse:.2f} minutes\")\n",
    "    print(f\"Regression Mean Absolute Error: {reg_mae:.2f} minutes\")\n",
    "    print(f\"Regression R² Score: {reg_r2:.4f}\")\n",
    "    \n",
    "    # Extract model coefficients\n",
    "    try:\n",
    "        feature_names = final_class_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "        log_reg_model = final_class_model.named_steps['classifier']\n",
    "\n",
    "        coefficients_df = plot_model_coefficients(\n",
    "            log_reg_model, \n",
    "            feature_names,\n",
    "            year=year,\n",
    "            top_n=20,\n",
    "            output_path=os.path.join(year_output_dir, 'plots', f'logistic_regression_coefficients_{year}.png')\n",
    "        )\n",
    "\n",
    "        coefficients_df.to_csv(\n",
    "            os.path.join(year_output_dir, 'metrics', f\"logistic_regression_coefficients_{year}.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Print top positive and negative coefficients\n",
    "        print(f\"\\nTop 5 features increasing delay probability for {year}:\")\n",
    "        print(coefficients_df[coefficients_df['Coefficient'] > 0].head(5))\n",
    "        \n",
    "        print(f\"\\nTop 5 features decreasing delay probability for {year}:\")\n",
    "        print(coefficients_df[coefficients_df['Coefficient'] < 0].head(5))\n",
    "        \n",
    "        # Analyze WEEK feature coefficients\n",
    "        week_features = [f for f in feature_names if 'WEEK' in f]\n",
    "        if week_features:\n",
    "            week_coefficients = coefficients_df[coefficients_df['Feature'].isin(week_features)]\n",
    "            print(f\"\\nDay of week coefficients for {year} (classification model):\")\n",
    "            print(week_coefficients)\n",
    "            \n",
    "            # Save separate plots\n",
    "            if len(week_coefficients) > 0:\n",
    "                plt.figure(figsize=(16, 10))\n",
    "                colors = ['red' if c < 0 else 'blue' for c in week_coefficients['Coefficient']]\n",
    "                sns.barplot(x='Coefficient', y='Feature', data=week_coefficients, palette=colors)\n",
    "                plt.title(f'Day of Week Coefficients (Logistic Regression - {year})')\n",
    "                plt.axvline(x=0, color='gray', linestyle='--')\n",
    "                plt.xlabel('Coefficient Value (+ increases delay, - decreases delay)')\n",
    "                plt.ylabel('Day of Week')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(year_output_dir, 'plots', f'day_of_week_coefficients_{year}.png'))\n",
    "                plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting logistic regression coefficients: {e}\")\n",
    "        coefficients_df = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        lin_reg_model = final_reg_model.named_steps['regressor']\n",
    "\n",
    "        lin_coefficients = lin_reg_model.coef_\n",
    "\n",
    "        lin_coef_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': lin_coefficients\n",
    "        })\n",
    "\n",
    "        lin_coef_df['AbsCoef'] = np.abs(lin_coef_df['Coefficient'])\n",
    "        lin_coef_df = lin_coef_df.sort_values('AbsCoef', ascending=False)\n",
    "\n",
    "        lin_coef_df.to_csv(\n",
    "            os.path.join(year_output_dir, 'metrics', f\"linear_regression_coefficients_{year}.csv\"), \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Plot top linear regression coefficients\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        top_coef = lin_coef_df.head(20)\n",
    "        colors = ['red' if c < 0 else 'blue' for c in top_coef['Coefficient']]\n",
    "        sns.barplot(x='Coefficient', y='Feature', data=top_coef, palette=colors)\n",
    "        \n",
    "        plt.title(f'Top 20 Most Influential Features for Delay Duration (Linear Regression - {year})')\n",
    "        plt.axvline(x=0, color='gray', linestyle='--')\n",
    "        plt.xlabel('Coefficient (minutes of delay)')\n",
    "        plt.ylabel('Feature')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(year_output_dir, 'plots', f'linear_regression_coefficients_{year}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Print top features\n",
    "        print(f\"\\nTop 5 features increasing delay duration for {year}:\")\n",
    "        print(lin_coef_df[lin_coef_df['Coefficient'] > 0].head(5))\n",
    "        \n",
    "        print(f\"\\nTop 5 features decreasing delay duration for {year}:\")\n",
    "        print(lin_coef_df[lin_coef_df['Coefficient'] < 0].head(5))\n",
    "        \n",
    "        if week_features:\n",
    "            week_lin_coefficients = lin_coef_df[lin_coef_df['Feature'].isin(week_features)]\n",
    "            print(f\"\\nDay of week coefficients for {year} (regression model):\")\n",
    "            print(week_lin_coefficients)\n",
    "\n",
    "            if len(week_lin_coefficients) > 0:\n",
    "                plt.figure(figsize=(16, 10))\n",
    "                colors = ['red' if c < 0 else 'blue' for c in week_lin_coefficients['Coefficient']]\n",
    "                sns.barplot(x='Coefficient', y='Feature', data=week_lin_coefficients, palette=colors)\n",
    "                plt.title(f'Day of Week Coefficients (Linear Regression - {year})')\n",
    "                plt.axvline(x=0, color='gray', linestyle='--')\n",
    "                plt.xlabel('Coefficient Value (minutes of delay)')\n",
    "                plt.ylabel('Day of Week')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(year_output_dir, 'plots', f'day_of_week_regression_coefficients_{year}.png'))\n",
    "                plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting linear regression coefficients: {e}\")\n",
    "        lin_coef_df = pd.DataFrame()\n",
    "    \n",
    "    # Create visualization plots\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.heatmap(class_cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['Not Delayed', 'Delayed'],\n",
    "               yticklabels=['Not Delayed', 'Delayed'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Delay Classification Confusion Matrix ({year})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(year_output_dir, 'plots', f'confusion_matrix_{year}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot ROC curve for classification model\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    fpr, tpr, _ = roc_curve(y_test_class, y_prob_class)\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {class_roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for Delay Classification ({year})')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(year_output_dir, 'plots', f'roc_curve_{year}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Create a scatterplot with limited points for clarity\n",
    "    max_points = 5000\n",
    "    if len(y_test_reg) > max_points:\n",
    "        idx = np.random.choice(len(y_test_reg), max_points, replace=False)\n",
    "        sample_actual = y_test_reg.iloc[idx]\n",
    "        sample_pred = y_pred_reg[idx]\n",
    "    else:\n",
    "        sample_actual = y_test_reg\n",
    "        sample_pred = y_pred_reg\n",
    "    \n",
    "    plt.scatter(sample_actual, sample_pred, alpha=0.3)\n",
    "    \n",
    "    max_val = max(sample_actual.max(), sample_pred.max())\n",
    "    min_val = min(sample_actual.min(), sample_pred.min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.xlabel('Actual Delay (minutes)')\n",
    "    plt.ylabel('Predicted Delay (minutes)')\n",
    "    plt.title(f'Actual vs Predicted Delay ({year})')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(year_output_dir, 'plots', f'actual_vs_predicted_{year}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot delay prediction error distribution\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    prediction_errors = y_test_reg - y_pred_reg\n",
    "    sns.histplot(prediction_errors, bins=50, kde=True)\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    plt.xlabel('Prediction Error (minutes)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Delay Prediction Error Distribution ({year})')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(year_output_dir, 'plots', f'error_distribution_{year}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot delay by time of day\n",
    "    if 'TIME_BLOCK' in flight_data.columns:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        time_delay = flight_data.groupby('TIME_BLOCK')['DEP_DELAY'].agg(['mean', 'count']).reset_index()\n",
    "        time_delay = time_delay.sort_values('mean', ascending=False)\n",
    "\n",
    "        ax1 = plt.subplot(111)\n",
    "        bars = sns.barplot(x='TIME_BLOCK', y='mean', data=time_delay, ax=ax1)\n",
    "\n",
    "        for bar, mean in zip(bars.patches, time_delay['mean']):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                    f'{mean:.1f}', ha='center', va='bottom')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(time_delay.index, time_delay['count'], 'ro-', linewidth=2)\n",
    "\n",
    "        ax1.set_xlabel('Time Block')\n",
    "        ax1.set_ylabel('Mean Delay (minutes)')\n",
    "        ax2.set_ylabel('Number of Flights', color='r')\n",
    "        plt.title(f'Mean Delay by Time of Day ({year})')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(year_output_dir, 'plots', f'delay_by_time_{year}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot delay by day\n",
    "    if 'WEEK' in flight_data.columns:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        \n",
    "        week_delay = flight_data.groupby('WEEK')['DEP_DELAY'].mean().reset_index()\n",
    "        \n",
    "        week_delay_rate = flight_data.groupby('WEEK')['IS_DELAYED'].mean().reset_index()\n",
    "        week_delay_rate['IS_DELAYED'] = week_delay_rate['IS_DELAYED'] * 100  # Convert to percentage\n",
    "        \n",
    "        if all(day in week_delay['WEEK'].values for day in day_order):\n",
    "            week_delay['WEEK'] = pd.Categorical(week_delay['WEEK'], categories=day_order, ordered=True)\n",
    "            week_delay = week_delay.sort_values('WEEK')\n",
    "            \n",
    "            week_delay_rate['WEEK'] = pd.Categorical(week_delay_rate['WEEK'], categories=day_order, ordered=True)\n",
    "            week_delay_rate = week_delay_rate.sort_values('WEEK')\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
    "        \n",
    "        bars1 = sns.barplot(x='WEEK', y='DEP_DELAY', data=week_delay, ax=ax1)\n",
    "        \n",
    "        for bar, mean in zip(bars1.patches, week_delay['DEP_DELAY']):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, mean + 0.5, \n",
    "                    f'{mean:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "        ax1.set_xlabel('Day of Week')\n",
    "        ax1.set_ylabel('Mean Delay (minutes)')\n",
    "        ax1.set_title(f'Mean Delay by Day of Week ({year})')\n",
    "        \n",
    "        # Plot delay rate by day\n",
    "        bars2 = sns.barplot(x='WEEK', y='IS_DELAYED', data=week_delay_rate, ax=ax2)\n",
    "\n",
    "        for bar, rate in zip(bars2.patches, week_delay_rate['IS_DELAYED']):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, rate + 0.5, \n",
    "                    f'{rate:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        ax2.set_xlabel('Day of Week')\n",
    "        ax2.set_ylabel('Delay Rate (%)')\n",
    "        ax2.set_title(f'Delay Rate by Day of Week ({year})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(year_output_dir, 'plots', f'delay_by_week_{year}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a heatmap showing delay patterns by day and hour\n",
    "        if 'DEP_HOUR' in flight_data.columns:\n",
    "            plt.figure(figsize=(16, 10))\n",
    "\n",
    "            delay_heatmap = flight_data.pivot_table(\n",
    "                values='DEP_DELAY', \n",
    "                index='WEEK',\n",
    "                columns='DEP_HOUR', \n",
    "                aggfunc='mean'\n",
    "            )\n",
    "\n",
    "            if all(day in delay_heatmap.index for day in day_order):\n",
    "                delay_heatmap = delay_heatmap.reindex(day_order)\n",
    "\n",
    "            ax = sns.heatmap(delay_heatmap, cmap='YlOrRd', annot=True, fmt='.1f', linewidths=.5)\n",
    "            \n",
    "            plt.title(f'Mean Delay by Day of Week and Hour of Day ({year})')\n",
    "            plt.xlabel('Hour of Day')\n",
    "            plt.ylabel('Day of Week')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(year_output_dir, 'plots', f'delay_heatmap_{year}.png'))\n",
    "            plt.close()\n",
    "\n",
    "    if 'IS_REDEYE' in flight_data.columns:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        redeye_delay = flight_data[flight_data['IS_REDEYE'] == 1]['DEP_DELAY'].mean()\n",
    "        non_redeye_delay = flight_data[flight_data['IS_REDEYE'] == 0]['DEP_DELAY'].mean()\n",
    "        \n",
    "        redeye_count = (flight_data['IS_REDEYE'] == 1).sum()\n",
    "        non_redeye_count = (flight_data['IS_REDEYE'] == 0).sum()\n",
    "        \n",
    "        categories = ['Non-Red-Eye', 'Red-Eye']\n",
    "        delays = [non_redeye_delay, redeye_delay]\n",
    "        counts = [non_redeye_count, redeye_count]\n",
    "        \n",
    "        bars = plt.bar(categories, delays, color=['skyblue', 'navy'])\n",
    "\n",
    "        for i, (bar, delay, count) in enumerate(zip(bars, delays, counts)):\n",
    "            plt.text(i, delay + 0.5, f\"{delay:.2f} min\\n({count:,} flights)\", \n",
    "                     ha='center', va='bottom')\n",
    "        \n",
    "        plt.ylabel('Mean Delay (minutes)')\n",
    "        plt.title(f'Mean Delay Comparison: Red-Eye vs. Non-Red-Eye Flights ({year})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(year_output_dir, 'plots', f'redeye_delay_comparison_{year}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Save models\n",
    "    dump(final_class_model, os.path.join(year_output_dir, f\"logistic_regression_model_{year}.joblib\"))\n",
    "    dump(final_reg_model, os.path.join(year_output_dir, f\"linear_regression_model_{year}.joblib\"))\n",
    "    print(f\"Models saved to {year_output_dir}\")\n",
    "    \n",
    "    # Create summary metrics\n",
    "    metrics = {\n",
    "        'model_name': f'logistic_regression_{year}',\n",
    "        'year': year,\n",
    "        \n",
    "        # Dataset metrics\n",
    "        'total_flights': len(flight_data),\n",
    "        'delayed_flights_rate': flight_data['IS_DELAYED'].mean() * 100,\n",
    "        'mean_delay': flight_data['DEP_DELAY'].mean(),\n",
    "        'median_delay': flight_data['DEP_DELAY'].median(),\n",
    "        'max_delay': flight_data['DEP_DELAY'].max(),\n",
    "        'min_delay': flight_data['DEP_DELAY'].min(),\n",
    "        \n",
    "        # Classification metrics\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'class_roc_auc': class_roc_auc,\n",
    "        'class_precision': class_report['1']['precision'],\n",
    "        'class_recall': class_report['1']['recall'],\n",
    "        'class_f1': class_report['1']['f1-score'],\n",
    "        'class_training_time': class_model_training_time,\n",
    "        \n",
    "        # Regression metrics\n",
    "        'reg_mse': reg_mse,\n",
    "        'reg_rmse': reg_rmse,\n",
    "        'reg_mae': reg_mae,\n",
    "        'reg_r2': reg_r2,\n",
    "        'reg_training_time': reg_model_training_time,\n",
    "        \n",
    "        # Red-eye metrics\n",
    "        'redeye_count': redeye_count,\n",
    "        'redeye_percentage': redeye_count / len(flight_data) * 100,\n",
    "        'redeye_mean_delay': redeye_delay,\n",
    "        'non_redeye_mean_delay': non_redeye_delay,\n",
    "        \n",
    "        'status': 'success',\n",
    "        'total_processing_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    # Save top important coefficients\n",
    "    if not coefficients_df.empty:\n",
    "        top_positive = coefficients_df[coefficients_df['Coefficient'] > 0].head(5)\n",
    "        for i, (_, row) in enumerate(top_positive.iterrows()):\n",
    "            metrics[f'top_positive_coef_{i+1}'] = row['Feature']\n",
    "            metrics[f'top_positive_coef_{i+1}_value'] = float(row['Coefficient'])\n",
    "        \n",
    "        top_negative = coefficients_df[coefficients_df['Coefficient'] < 0].head(5)\n",
    "        for i, (_, row) in enumerate(top_negative.iterrows()):\n",
    "            metrics[f'top_negative_coef_{i+1}'] = row['Feature']\n",
    "            metrics[f'top_negative_coef_{i+1}_value'] = float(row['Coefficient'])\n",
    "        \n",
    "        if 'week_coefficients' in locals() and not week_coefficients.empty:\n",
    "            metrics['week_coefficients'] = convert_to_serializable(week_coefficients.to_dict('records'))\n",
    "    \n",
    "    if not lin_coef_df.empty:\n",
    "        top_lin_coef = lin_coef_df.head(10)\n",
    "        for i, (_, row) in enumerate(top_lin_coef.iterrows()):\n",
    "            metrics[f'top_linear_coef_{i+1}'] = row['Feature']\n",
    "            metrics[f'top_linear_coef_{i+1}_value'] = float(row['Coefficient'])\n",
    "        \n",
    "        if 'week_lin_coefficients' in locals() and not week_lin_coefficients.empty:\n",
    "            metrics['week_regression_coefficients'] = convert_to_serializable(week_lin_coefficients.to_dict('records'))\n",
    "    \n",
    "    # Save metrics to JSON\n",
    "    import json\n",
    "    \n",
    "    serializable_metrics = convert_to_serializable(metrics)\n",
    "    with open(os.path.join(year_output_dir, 'metrics', f'model_metrics_{year}.json'), 'w') as f:\n",
    "        json.dump(serializable_metrics, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nLogistic Regression model training for {year} complete! Total processing time: {metrics['total_processing_time']:.2f} seconds\")\n",
    "    return metrics"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:16:45.546947Z",
     "start_time": "2025-04-17T22:08:54.510614Z"
    }
   },
   "source": [
    "# Function to compare models across years\n",
    "def compare_year_models(all_results):\n",
    "    print(\"\\nComparing models across years...\")\n",
    "    \n",
    "    if not all_results or len(all_results) < 2:\n",
    "        print(\"Not enough year models to compare.\")\n",
    "        return\n",
    "    \n",
    "    # Create a comparison directory\n",
    "    comparison_dir = os.path.join(output_dir, 'comparison')\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract years and sort them\n",
    "    years = sorted([r['year'] for r in all_results])\n",
    "    \n",
    "    # Create DataFrames for different metrics\n",
    "    class_metrics = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'Accuracy (%)': [r['class_accuracy'] for r in all_results],\n",
    "        'AUC': [r['class_roc_auc'] for r in all_results],\n",
    "        'Precision': [r['class_precision'] for r in all_results],\n",
    "        'Recall': [r['class_recall'] for r in all_results],\n",
    "        'F1 Score': [r['class_f1'] for r in all_results],\n",
    "    })\n",
    "    \n",
    "    reg_metrics = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'RMSE (min)': [r['reg_rmse'] for r in all_results],\n",
    "        'MAE (min)': [r['reg_mae'] for r in all_results],\n",
    "        'R² Score': [r['reg_r2'] for r in all_results],\n",
    "    })\n",
    "    \n",
    "    delay_stats = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'Mean Delay (min)': [r['mean_delay'] for r in all_results],\n",
    "        'Delay Rate (%)': [r['delayed_flights_rate'] for r in all_results],\n",
    "        'Total Flights': [r['total_flights'] for r in all_results],\n",
    "    })\n",
    "    \n",
    "    # Plot classification metrics\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.15\n",
    "    r1 = np.arange(len(years))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "    r3 = [x + bar_width for x in r2]\n",
    "    r4 = [x + bar_width for x in r3]\n",
    "    r5 = [x + bar_width for x in r4]\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(r1, class_metrics['Accuracy (%)'] / 100, width=bar_width, label='Accuracy', color='blue')\n",
    "    plt.bar(r2, class_metrics['AUC'], width=bar_width, label='AUC', color='green')\n",
    "    plt.bar(r3, class_metrics['Precision'], width=bar_width, label='Precision', color='red')\n",
    "    plt.bar(r4, class_metrics['Recall'], width=bar_width, label='Recall', color='purple')\n",
    "    plt.bar(r5, class_metrics['F1 Score'], width=bar_width, label='F1 Score', color='orange')\n",
    "    \n",
    "    # Add texts on bars\n",
    "    for i, r in enumerate([r1, r2, r3, r4, r5]):\n",
    "        values = class_metrics.iloc[:, i+1].values\n",
    "        if i == 0:\n",
    "            values = values / 100\n",
    "        for j, v in enumerate(values):\n",
    "            plt.text(r[j], v + 0.01, f'{v:.2f}' if i > 0 else f'{v*100:.1f}%', \n",
    "                    ha='center', va='bottom', rotation=0, fontsize=8)\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Classification Metrics by Year')\n",
    "    plt.xticks([r + 2*bar_width for r in range(len(years))], years)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, 'classification_metrics_by_year.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot regression metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Plot RMSE and MAE\n",
    "    x = np.arange(len(years))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, reg_metrics['RMSE (min)'], width, label='RMSE')\n",
    "    ax1.bar(x + width/2, reg_metrics['MAE (min)'], width, label='MAE')\n",
    "\n",
    "    for i, v in enumerate(reg_metrics['RMSE (min)']):\n",
    "        ax1.text(i - width/2, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(reg_metrics['MAE (min)']):\n",
    "        ax1.text(i + width/2, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Minutes')\n",
    "    ax1.set_title('Regression Error Metrics')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(years)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot R² Score\n",
    "    bars = ax2.bar(years, reg_metrics['R² Score'], color='green')\n",
    "\n",
    "    for bar, value in zip(bars, reg_metrics['R² Score']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, value + 0.01, f'{value:.3f}', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.set_ylabel('R² Score')\n",
    "    ax2.set_title('Regression R² Score')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, 'regression_metrics_by_year.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot delay statistics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
    "\n",
    "    bars1 = ax1.bar(years, delay_stats['Mean Delay (min)'], color='blue')\n",
    "\n",
    "    for bar, value in zip(bars1, delay_stats['Mean Delay (min)']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, value + 0.3, f'{value:.1f}', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Minutes')\n",
    "    ax1.set_title('Mean Delay by Year')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Plot delay rate\n",
    "    bars2 = ax2.bar(years, delay_stats['Delay Rate (%)'], color='red')\n",
    "\n",
    "    for bar, value in zip(bars2, delay_stats['Delay Rate (%)']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, value + 0.5, f'{value:.1f}%', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.set_title('Delay Rate by Year')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, 'delay_stats_by_year.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot total flights\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    bars = plt.bar(years, delay_stats['Total Flights'], color='purple')\n",
    "\n",
    "    for bar, value in zip(bars, delay_stats['Total Flights']):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, value + 100, f'{value:,}', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Flights')\n",
    "    plt.title('Total Flights by Year')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, 'total_flights_by_year.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a summary table for all metrics\n",
    "    summary = pd.concat([\n",
    "        delay_stats.set_index('Year'),\n",
    "        class_metrics.set_index('Year').iloc[:, 1:],\n",
    "        reg_metrics.set_index('Year').iloc[:, 1:]\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Save the summary to CSV\n",
    "    summary.to_csv(os.path.join(comparison_dir, 'year_model_comparison.csv'))\n",
    "    print(f\"Comparison summary saved to {os.path.join(comparison_dir, 'year_model_comparison.csv')}\")\n",
    "    \n",
    "    try:\n",
    "        week_coefficients_by_year = {}\n",
    "        \n",
    "        for r in all_results:\n",
    "            year = r['year']\n",
    "            if 'week_coefficients' in r:\n",
    "                week_coefficients_by_year[year] = r['week_coefficients']\n",
    "        \n",
    "        if week_coefficients_by_year:\n",
    "            with open(os.path.join(comparison_dir, 'day_of_week_coefficients_by_year.json'), 'w') as f:\n",
    "                json.dump(convert_to_serializable(week_coefficients_by_year), f, indent=4)\n",
    "            \n",
    "            print(f\"Day of week coefficients by year saved to {os.path.join(comparison_dir, 'day_of_week_coefficients_by_year.json')}\")\n",
    "            \n",
    "            try:\n",
    "                day_data = []\n",
    "                \n",
    "                for year, coeffs in week_coefficients_by_year.items():\n",
    "                    for coeff in coeffs:\n",
    "                        if 'Feature' in coeff and 'Coefficient' in coeff:\n",
    "                            feature = coeff['Feature']\n",
    "                            if 'WEEK_' in feature:\n",
    "                                day = feature.split('WEEK_')[1]\n",
    "                                day_data.append({\n",
    "                                    'Year': year,\n",
    "                                    'Day': day,\n",
    "                                    'Coefficient': coeff['Coefficient']\n",
    "                                })\n",
    "                \n",
    "                if day_data:\n",
    "                    day_df = pd.DataFrame(day_data)\n",
    "                    \n",
    "                    day_pivot = day_df.pivot(index='Day', columns='Year', values='Coefficient')\n",
    "                    \n",
    "                    day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "                    day_pivot = day_pivot.reindex([d for d in day_order if d in day_pivot.index])\n",
    "                    \n",
    "                    # Plot heatmap\n",
    "                    plt.figure(figsize=(16, 10))\n",
    "                    sns.heatmap(day_pivot, cmap='RdBu_r', center=0, annot=True, fmt='.3f')\n",
    "                    plt.title('Day of Week Coefficients Across Years (Logistic Regression)')\n",
    "                    plt.xlabel('Year')\n",
    "                    plt.ylabel('Day of Week')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(comparison_dir, 'day_of_week_coefficients_heatmap.png'))\n",
    "                    plt.close()\n",
    "                    \n",
    "                    # Create bar chart showing coefficient evolution for each day\n",
    "                    plt.figure(figsize=(16, 10))\n",
    "                    \n",
    "                    for i, day in enumerate(day_pivot.index):\n",
    "                        plt.subplot(len(day_pivot.index), 1, i+1)\n",
    "                        \n",
    "                        year_vals = day_pivot.loc[day].index.astype(str)\n",
    "                        coef_vals = day_pivot.loc[day].values\n",
    "                        \n",
    "                        bars = plt.bar(year_vals, coef_vals, \n",
    "                                      color=['red' if c < 0 else 'blue' for c in coef_vals])\n",
    "\n",
    "                        for bar, val in zip(bars, coef_vals):\n",
    "                            plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                                    val + (0.01 if val >= 0 else -0.04), \n",
    "                                    f'{val:.3f}', ha='center')\n",
    "                        \n",
    "                        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "                        plt.title(f'{day}')\n",
    "                        plt.ylim(min(day_pivot.values.min() * 1.1, -0.05), \n",
    "                                max(day_pivot.values.max() * 1.1, 0.05))\n",
    "\n",
    "                        if i == len(day_pivot.index) - 1:\n",
    "                            plt.xlabel('Year')\n",
    "                    \n",
    "                    plt.suptitle('Evolution of Day of Week Coefficients Across Years', fontsize=16, y=0.98)\n",
    "                    plt.tight_layout()\n",
    "                    plt.subplots_adjust(top=0.93)\n",
    "                    plt.savefig(os.path.join(comparison_dir, 'day_of_week_coefficients_evolution.png'))\n",
    "                    plt.close()\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating day of week visualization: {e}\")\n",
    "\n",
    "        feature_comparison = {\n",
    "            'positive': {},\n",
    "            'negative': {}\n",
    "        }\n",
    "        \n",
    "        for r in all_results:\n",
    "            year = r['year']\n",
    "            feature_comparison['positive'][year] = []\n",
    "            feature_comparison['negative'][year] = []\n",
    "            \n",
    "            # Collect top positive features\n",
    "            for i in range(1, 6):\n",
    "                feat_key = f'top_positive_coef_{i}'\n",
    "                val_key = f'top_positive_coef_{i}_value'\n",
    "                if feat_key in r and val_key in r:\n",
    "                    feature_comparison['positive'][year].append({\n",
    "                        'feature': r[feat_key],\n",
    "                        'value': r[val_key]\n",
    "                    })\n",
    "            \n",
    "            # Collect top negative features\n",
    "            for i in range(1, 6):\n",
    "                feat_key = f'top_negative_coef_{i}'\n",
    "                val_key = f'top_negative_coef_{i}_value'\n",
    "                if feat_key in r and val_key in r:\n",
    "                    feature_comparison['negative'][year].append({\n",
    "                        'feature': r[feat_key],\n",
    "                        'value': r[val_key]\n",
    "                    })\n",
    "        \n",
    "        # Save feature comparison to JSON\n",
    "        with open(os.path.join(comparison_dir, 'feature_coefficient_comparison.json'), 'w') as f:\n",
    "            json.dump(convert_to_serializable(feature_comparison), f, indent=4)\n",
    "        \n",
    "        print(f\"Feature coefficient comparison saved to {os.path.join(comparison_dir, 'feature_coefficient_comparison.json')}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating feature importance comparison: {e}\")\n",
    "    \n",
    "    print(\"Year model comparison completed!\")\n",
    "\n",
    "# Main execution\n",
    "all_results = []\n",
    "\n",
    "# Process each year's file separately\n",
    "for file_path in flight_files:\n",
    "    year = extract_year_from_filename(file_path)\n",
    "    results = train_year_model(year, file_path)\n",
    "    \n",
    "    if results:\n",
    "        all_results.append(results)\n",
    "        print(f\"\\nModel for year {year} completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\nModel for year {year} failed.\")\n",
    "\n",
    "# After all individual models are trained, compare them\n",
    "if len(all_results) > 1:\n",
    "    compare_year_models(all_results)\n",
    "else:\n",
    "    print(\"\\nNot enough successful models to perform comparison.\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nYear-by-Year Model Training Summary:\")\n",
    "for year_result in all_results:\n",
    "    year = year_result['year']\n",
    "    print(f\"\\nYear {year}:\")\n",
    "    print(f\"  Total flights: {year_result['total_flights']:,}\")\n",
    "    print(f\"  Classification accuracy: {year_result['class_accuracy']:.2f}%\")\n",
    "    print(f\"  Classification AUC: {year_result['class_roc_auc']:.4f}\")\n",
    "    print(f\"  Regression RMSE: {year_result['reg_rmse']:.2f} minutes\")\n",
    "    print(f\"  Regression R²: {year_result['reg_r2']:.4f}\")\n",
    "    print(f\"  Mean delay: {year_result['mean_delay']:.2f} minutes\")\n",
    "    print(f\"  Delay rate: {year_result['delayed_flights_rate']:.2f}%\")\n",
    "    \n",
    "    if 'week_coefficients' in year_result:\n",
    "        print(f\"\\n  Day of week patterns for {year}:\")\n",
    "        \n",
    "        week_coeffs = sorted(year_result['week_coefficients'],\n",
    "                             key=lambda x: x['Coefficient'], \n",
    "                             reverse=True)\n",
    "        \n",
    "        for coeff in week_coeffs:\n",
    "            feature = coeff['Feature']\n",
    "            value = coeff['Coefficient']\n",
    "\n",
    "            if 'WEEK_' in feature:\n",
    "                day = feature.split('WEEK_')[1]\n",
    "                direction = \"increases\" if value > 0 else \"decreases\"\n",
    "                print(f\"    • {day}: {direction} delay probability by {abs(value):.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete! Check output directories for detailed results.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training model for year 2021\n",
      "================================================================================\n",
      "\n",
      "Processing May2021.csv...\n",
      "Years found in data: [2021]\n",
      "Months found in data: {5: 520059}\n",
      "Filtered to only May data: 520059 rows\n",
      "Filtered from 520059 to 171867 rows for top 30 airports\n",
      "Removed 485.0 cancelled flights, remaining: 171382\n",
      "Processing took: 7.59 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/171382 rows, matched 7633 flights with weather data\n",
      "Processed 20000/171382 rows, matched 15255 flights with weather data\n",
      "Processed 30000/171382 rows, matched 22811 flights with weather data\n",
      "Processed 40000/171382 rows, matched 30386 flights with weather data\n",
      "Processed 50000/171382 rows, matched 37926 flights with weather data\n",
      "Processed 60000/171382 rows, matched 45609 flights with weather data\n",
      "Processed 70000/171382 rows, matched 53275 flights with weather data\n",
      "Processed 80000/171382 rows, matched 60864 flights with weather data\n",
      "Processed 90000/171382 rows, matched 68367 flights with weather data\n",
      "Processed 100000/171382 rows, matched 75976 flights with weather data\n",
      "Processed 110000/171382 rows, matched 83600 flights with weather data\n",
      "Processed 120000/171382 rows, matched 91287 flights with weather data\n",
      "Processed 130000/171382 rows, matched 98785 flights with weather data\n",
      "Processed 140000/171382 rows, matched 106365 flights with weather data\n",
      "Processed 150000/171382 rows, matched 113975 flights with weather data\n",
      "Processed 160000/171382 rows, matched 121693 flights with weather data\n",
      "Processed 170000/171382 rows, matched 129253 flights with weather data\n",
      "Processed 171382/171382 rows, matched 130389 flights with weather data\n",
      "Matched weather data for 130389 flights (76.08%)\n",
      "Weather matching took: 189.03 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2021...\n",
      "Identified 2854 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 4648 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 7186 out of 171382 total flights (4.19%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 67291 flights (39.26%)\n",
      "  - Afternoon (12-18): 61243 flights (35.73%)\n",
      "  - Evening (18-24): 35957 flights (20.98%)\n",
      "  - Early Morning (0-6): 6891 flights (4.02%)\n",
      "\n",
      "Preparing delay data for 2021...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 54865/171382 (32.01%)\n",
      "On-time or early flights: 116517/171382 (67.99%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 6.99 minutes\n",
      "Median delay: -2.00 minutes\n",
      "Min delay: -35.00 minutes (negative means early departure)\n",
      "Max delay: 1800.00 minutes\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 355 flights (0.21%)\n",
      "  - Early: 116162 flights (67.78%)\n",
      "  - On Time: 31919 flights (18.62%)\n",
      "  - Moderate Delay: 15864 flights (9.26%)\n",
      "  - Significant Delay: 4268 flights (2.49%)\n",
      "  - Severe Delay: 2814 flights (1.64%)\n",
      "\n",
      "Creating time block features for 2021...\n",
      "\n",
      "Creating day features for 2021...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Monday: 28849 flights (16.83%)\n",
      "  - Sunday: 28404 flights (16.57%)\n",
      "  - Saturday: 25897 flights (15.11%)\n",
      "  - Friday: 23495 flights (13.71%)\n",
      "  - Thursday: 23320 flights (13.61%)\n",
      "  - Wednesday: 21034 flights (12.27%)\n",
      "  - Tuesday: 20383 flights (11.89%)\n",
      "\n",
      "Weekend flights: 54301 (31.68%)\n",
      "Weekday flights: 117081 (68.32%)\n",
      "\n",
      "Selecting features for delay prediction for 2021...\n",
      "Using categorical features: ['DAY_NAME', 'TIME_BLOCK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']\n",
      "Using numerical features: ['DISTANCE', 'PRCP']\n",
      "Training set size: (154243, 12)\n",
      "Test set size: (17139, 12)\n",
      "\n",
      "Training delay classification model for 2021 (Logistic Regression)...\n",
      "Classification model training took: 2.54 seconds\n",
      "\n",
      "Training delay regression model for 2021 (Linear Regression)...\n",
      "Regression model training took: 0.60 seconds\n",
      "\n",
      "Evaluating delay classification model for 2021...\n",
      "Classification Accuracy: 64.44%\n",
      "Classification ROC AUC: 0.6881\n",
      "Classification Precision (Delayed): 0.4598\n",
      "Classification Recall (Delayed): 0.6322\n",
      "Classification F1 Score (Delayed): 0.5324\n",
      "\n",
      "Evaluating delay regression model for 2021...\n",
      "Regression Mean Squared Error: 1466.82\n",
      "Regression Root Mean Squared Error: 38.30 minutes\n",
      "Regression Mean Absolute Error: 15.20 minutes\n",
      "Regression R² Score: 0.0364\n",
      "Coefficient plot saved to ./dep_delay_lr/year_2021\\plots\\logistic_regression_coefficients_2021.png\n",
      "\n",
      "Top 5 features increasing delay probability for 2021:\n",
      "                          Feature  Coefficient   AbsCoef\n",
      "25            cat__MKT_AIRLINE_WN     1.122959  1.122959\n",
      "34           cat__ORIGIN_IATA_DFW     0.910202  0.910202\n",
      "26           cat__ORIGIN_IATA_ATL     0.528307  0.528307\n",
      "16  cat__TIME_BLOCK_Night (18-21)     0.379852  0.379852\n",
      "33           cat__ORIGIN_IATA_DEN     0.379030  0.379030\n",
      "\n",
      "Top 5 features decreasing delay probability for 2021:\n",
      "                                Feature  Coefficient   AbsCoef\n",
      "15        cat__TIME_BLOCK_Morning (6-9)    -0.638036  0.638036\n",
      "32                 cat__ORIGIN_IATA_DCA    -0.605169  0.605169\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)    -0.568224  0.568224\n",
      "47                 cat__ORIGIN_IATA_MSP    -0.500554  0.500554\n",
      "24                  cat__MKT_AIRLINE_UA    -0.453958  0.453958\n",
      "\n",
      "Day of week coefficients for 2021 (classification model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "90  cat__IS_WEEKEND_0    -0.043772  0.043772\n",
      "91  cat__IS_WEEKEND_1    -0.005696  0.005696\n",
      "\n",
      "Top 5 features increasing delay duration for 2021:\n",
      "                 Feature  Coefficient    AbsCoef\n",
      "34  cat__ORIGIN_IATA_DFW    11.675740  11.675740\n",
      "64    cat__DEST_IATA_DFW     9.401759   9.401759\n",
      "25   cat__MKT_AIRLINE_WN     6.269340   6.269340\n",
      "1              num__PRCP     3.934285   3.934285\n",
      "69    cat__DEST_IATA_IAH     3.767004   3.767004\n",
      "\n",
      "Top 5 features decreasing delay duration for 2021:\n",
      "                                Feature  Coefficient   AbsCoef\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)    -5.753706  5.753706\n",
      "15        cat__TIME_BLOCK_Morning (6-9)    -4.477556  4.477556\n",
      "47                 cat__ORIGIN_IATA_MSP    -4.272130  4.272130\n",
      "18                  cat__MKT_AIRLINE_AS    -3.661885  3.661885\n",
      "28                 cat__ORIGIN_IATA_BNA    -3.151511  3.151511\n",
      "\n",
      "Day of week coefficients for 2021 (regression model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "91  cat__IS_WEEKEND_1    -0.069531  0.069531\n",
      "90  cat__IS_WEEKEND_0     0.069531  0.069531\n",
      "Models saved to ./dep_delay_lr/year_2021\n",
      "\n",
      "Logistic Regression model training for 2021 complete! Total processing time: 203.46 seconds\n",
      "\n",
      "Model for year 2021 completed successfully!\n",
      "\n",
      "================================================================================\n",
      "Training model for year 2022\n",
      "================================================================================\n",
      "\n",
      "Processing May2022.csv...\n",
      "Years found in data: [2022]\n",
      "Months found in data: {5: 602950}\n",
      "Filtered to only May data: 602950 rows\n",
      "Filtered from 602950 to 210079 rows for top 30 airports\n",
      "Removed 4659.0 cancelled flights, remaining: 205420\n",
      "Processing took: 3.90 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/205420 rows, matched 7065 flights with weather data\n",
      "Processed 20000/205420 rows, matched 14381 flights with weather data\n",
      "Processed 30000/205420 rows, matched 21409 flights with weather data\n",
      "Processed 40000/205420 rows, matched 28736 flights with weather data\n",
      "Processed 50000/205420 rows, matched 35905 flights with weather data\n",
      "Processed 60000/205420 rows, matched 43280 flights with weather data\n",
      "Processed 70000/205420 rows, matched 50286 flights with weather data\n",
      "Processed 80000/205420 rows, matched 57605 flights with weather data\n",
      "Processed 90000/205420 rows, matched 64757 flights with weather data\n",
      "Processed 100000/205420 rows, matched 72193 flights with weather data\n",
      "Processed 110000/205420 rows, matched 79374 flights with weather data\n",
      "Processed 120000/205420 rows, matched 86687 flights with weather data\n",
      "Processed 130000/205420 rows, matched 93773 flights with weather data\n",
      "Processed 140000/205420 rows, matched 101221 flights with weather data\n",
      "Processed 150000/205420 rows, matched 108364 flights with weather data\n",
      "Processed 160000/205420 rows, matched 115722 flights with weather data\n",
      "Processed 170000/205420 rows, matched 122731 flights with weather data\n",
      "Processed 180000/205420 rows, matched 130269 flights with weather data\n",
      "Processed 190000/205420 rows, matched 137386 flights with weather data\n",
      "Processed 200000/205420 rows, matched 144660 flights with weather data\n",
      "Processed 205420/205420 rows, matched 148501 flights with weather data\n",
      "Matched weather data for 148501 flights (72.29%)\n",
      "Weather matching took: 75.02 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2022...\n",
      "Identified 6093 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 7457 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 13088 out of 205420 total flights (6.37%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 78343 flights (38.14%)\n",
      "  - Afternoon (12-18): 69688 flights (33.92%)\n",
      "  - Evening (18-24): 46622 flights (22.70%)\n",
      "  - Early Morning (0-6): 10767 flights (5.24%)\n",
      "\n",
      "Preparing delay data for 2022...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 91269/205420 (44.43%)\n",
      "On-time or early flights: 114151/205420 (55.57%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 14.56 minutes\n",
      "Median delay: 0.00 minutes\n",
      "Min delay: -34.00 minutes (negative means early departure)\n",
      "Max delay: 2109.00 minutes\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 208 flights (0.10%)\n",
      "  - Early: 113943 flights (55.47%)\n",
      "  - On Time: 44584 flights (21.70%)\n",
      "  - Moderate Delay: 31050 flights (15.12%)\n",
      "  - Significant Delay: 9754 flights (4.75%)\n",
      "  - Severe Delay: 5881 flights (2.86%)\n",
      "\n",
      "Creating time block features for 2022...\n",
      "\n",
      "Creating day features for 2022...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Monday: 34057 flights (16.58%)\n",
      "  - Tuesday: 33242 flights (16.18%)\n",
      "  - Sunday: 33153 flights (16.14%)\n",
      "  - Thursday: 28003 flights (13.63%)\n",
      "  - Friday: 26923 flights (13.11%)\n",
      "  - Wednesday: 26859 flights (13.08%)\n",
      "  - Saturday: 23183 flights (11.29%)\n",
      "\n",
      "Weekend flights: 56336 (27.42%)\n",
      "Weekday flights: 149084 (72.58%)\n",
      "\n",
      "Selecting features for delay prediction for 2022...\n",
      "Using categorical features: ['DAY_NAME', 'TIME_BLOCK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']\n",
      "Using numerical features: ['DISTANCE', 'PRCP']\n",
      "Training set size: (184878, 12)\n",
      "Test set size: (20542, 12)\n",
      "\n",
      "Training delay classification model for 2022 (Logistic Regression)...\n",
      "Classification model training took: 2.08 seconds\n",
      "\n",
      "Training delay regression model for 2022 (Linear Regression)...\n",
      "Regression model training took: 0.71 seconds\n",
      "\n",
      "Evaluating delay classification model for 2022...\n",
      "Classification Accuracy: 63.29%\n",
      "Classification ROC AUC: 0.6861\n",
      "Classification Precision (Delayed): 0.5771\n",
      "Classification Recall (Delayed): 0.6505\n",
      "Classification F1 Score (Delayed): 0.6116\n",
      "\n",
      "Evaluating delay regression model for 2022...\n",
      "Regression Mean Squared Error: 2436.22\n",
      "Regression Root Mean Squared Error: 49.36 minutes\n",
      "Regression Mean Absolute Error: 22.78 minutes\n",
      "Regression R² Score: 0.0331\n",
      "Coefficient plot saved to ./dep_delay_lr/year_2022\\plots\\logistic_regression_coefficients_2022.png\n",
      "\n",
      "Top 5 features increasing delay probability for 2022:\n",
      "                               Feature  Coefficient   AbsCoef\n",
      "25                 cat__MKT_AIRLINE_WN     0.801874  0.801874\n",
      "16       cat__TIME_BLOCK_Night (18-21)     0.618828  0.618828\n",
      "36                cat__ORIGIN_IATA_EWR     0.589460  0.589460\n",
      "13  cat__TIME_BLOCK_Late Night (21-24)     0.568330  0.568330\n",
      "11     cat__TIME_BLOCK_Evening (15-18)     0.536049  0.536049\n",
      "\n",
      "Top 5 features decreasing delay probability for 2022:\n",
      "                                Feature  Coefficient   AbsCoef\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)    -1.104447  1.104447\n",
      "15        cat__TIME_BLOCK_Morning (6-9)    -0.788898  0.788898\n",
      "51                 cat__ORIGIN_IATA_SAN    -0.495382  0.495382\n",
      "17                  cat__MKT_AIRLINE_AA    -0.388864  0.388864\n",
      "24                  cat__MKT_AIRLINE_UA    -0.345805  0.345805\n",
      "\n",
      "Day of week coefficients for 2022 (classification model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "91  cat__IS_WEEKEND_1     0.090611  0.090611\n",
      "90  cat__IS_WEEKEND_0    -0.059756  0.059756\n",
      "\n",
      "Top 5 features increasing delay duration for 2022:\n",
      "                               Feature  Coefficient    AbsCoef\n",
      "36                cat__ORIGIN_IATA_EWR    13.339541  13.339541\n",
      "66                  cat__DEST_IATA_EWR    10.496135  10.496135\n",
      "16       cat__TIME_BLOCK_Night (18-21)     9.112023   9.112023\n",
      "13  cat__TIME_BLOCK_Late Night (21-24)     7.852200   7.852200\n",
      "22                 cat__MKT_AIRLINE_G4     7.063815   7.063815\n",
      "\n",
      "Top 5 features decreasing delay duration for 2022:\n",
      "                                Feature  Coefficient    AbsCoef\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)   -11.386869  11.386869\n",
      "15        cat__TIME_BLOCK_Morning (6-9)    -9.086661   9.086661\n",
      "51                 cat__ORIGIN_IATA_SAN    -5.101999   5.101999\n",
      "18                  cat__MKT_AIRLINE_AS    -5.013776   5.013776\n",
      "14       cat__TIME_BLOCK_Mid-Day (9-12)    -4.840098   4.840098\n",
      "\n",
      "Day of week coefficients for 2022 (regression model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "91  cat__IS_WEEKEND_1     1.593162  1.593162\n",
      "90  cat__IS_WEEKEND_0    -1.593162  1.593162\n",
      "Models saved to ./dep_delay_lr/year_2022\n",
      "\n",
      "Logistic Regression model training for 2022 complete! Total processing time: 85.12 seconds\n",
      "\n",
      "Model for year 2022 completed successfully!\n",
      "\n",
      "================================================================================\n",
      "Training model for year 2023\n",
      "================================================================================\n",
      "\n",
      "Processing May2023.csv...\n",
      "Years found in data: [2023]\n",
      "Months found in data: {5: 616630}\n",
      "Filtered to only May data: 616630 rows\n",
      "Filtered from 616630 to 220469 rows for top 30 airports\n",
      "Removed 1293.0 cancelled flights, remaining: 219176\n",
      "Processing took: 3.57 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/219176 rows, matched 7071 flights with weather data\n",
      "Processed 20000/219176 rows, matched 14364 flights with weather data\n",
      "Processed 30000/219176 rows, matched 21626 flights with weather data\n",
      "Processed 40000/219176 rows, matched 28902 flights with weather data\n",
      "Processed 50000/219176 rows, matched 36373 flights with weather data\n",
      "Processed 60000/219176 rows, matched 43539 flights with weather data\n",
      "Processed 70000/219176 rows, matched 50884 flights with weather data\n",
      "Processed 80000/219176 rows, matched 58144 flights with weather data\n",
      "Processed 90000/219176 rows, matched 65463 flights with weather data\n",
      "Processed 100000/219176 rows, matched 72856 flights with weather data\n",
      "Processed 110000/219176 rows, matched 79891 flights with weather data\n",
      "Processed 120000/219176 rows, matched 86951 flights with weather data\n",
      "Processed 130000/219176 rows, matched 93797 flights with weather data\n",
      "Processed 140000/219176 rows, matched 101252 flights with weather data\n",
      "Processed 150000/219176 rows, matched 108560 flights with weather data\n",
      "Processed 160000/219176 rows, matched 115748 flights with weather data\n",
      "Processed 170000/219176 rows, matched 123276 flights with weather data\n",
      "Processed 180000/219176 rows, matched 130383 flights with weather data\n",
      "Processed 190000/219176 rows, matched 137913 flights with weather data\n",
      "Processed 200000/219176 rows, matched 145101 flights with weather data\n",
      "Processed 210000/219176 rows, matched 152449 flights with weather data\n",
      "Processed 219176/219176 rows, matched 159344 flights with weather data\n",
      "Matched weather data for 159344 flights (72.70%)\n",
      "Weather matching took: 83.34 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2023...\n",
      "Identified 6485 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 8863 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 15095 out of 219176 total flights (6.89%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 82862 flights (37.81%)\n",
      "  - Afternoon (12-18): 73196 flights (33.40%)\n",
      "  - Evening (18-24): 51478 flights (23.49%)\n",
      "  - Early Morning (0-6): 11640 flights (5.31%)\n",
      "\n",
      "Preparing delay data for 2023...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 87833/219176 (40.07%)\n",
      "On-time or early flights: 131343/219176 (59.93%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 12.35 minutes\n",
      "Median delay: -1.00 minutes\n",
      "Min delay: -29.00 minutes (negative means early departure)\n",
      "Max delay: 3221.00 minutes\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 428 flights (0.20%)\n",
      "  - Early: 130915 flights (59.73%)\n",
      "  - On Time: 44120 flights (20.13%)\n",
      "  - Moderate Delay: 29279 flights (13.36%)\n",
      "  - Significant Delay: 8888 flights (4.06%)\n",
      "  - Severe Delay: 5546 flights (2.53%)\n",
      "\n",
      "Creating time block features for 2023...\n",
      "\n",
      "Creating day features for 2023...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Monday: 36506 flights (16.66%)\n",
      "  - Wednesday: 35633 flights (16.26%)\n",
      "  - Tuesday: 35382 flights (16.14%)\n",
      "  - Thursday: 29268 flights (13.35%)\n",
      "  - Friday: 29138 flights (13.29%)\n",
      "  - Sunday: 28009 flights (12.78%)\n",
      "  - Saturday: 25240 flights (11.52%)\n",
      "\n",
      "Weekend flights: 53249 (24.30%)\n",
      "Weekday flights: 165927 (75.70%)\n",
      "\n",
      "Selecting features for delay prediction for 2023...\n",
      "Using categorical features: ['DAY_NAME', 'TIME_BLOCK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']\n",
      "Using numerical features: ['DISTANCE', 'PRCP']\n",
      "Training set size: (197258, 12)\n",
      "Test set size: (21918, 12)\n",
      "\n",
      "Training delay classification model for 2023 (Logistic Regression)...\n",
      "Classification model training took: 2.13 seconds\n",
      "\n",
      "Training delay regression model for 2023 (Linear Regression)...\n",
      "Regression model training took: 0.74 seconds\n",
      "\n",
      "Evaluating delay classification model for 2023...\n",
      "Classification Accuracy: 64.26%\n",
      "Classification ROC AUC: 0.7026\n",
      "Classification Precision (Delayed): 0.5450\n",
      "Classification Recall (Delayed): 0.6550\n",
      "Classification F1 Score (Delayed): 0.5950\n",
      "\n",
      "Evaluating delay regression model for 2023...\n",
      "Regression Mean Squared Error: 2457.66\n",
      "Regression Root Mean Squared Error: 49.57 minutes\n",
      "Regression Mean Absolute Error: 21.41 minutes\n",
      "Regression R² Score: 0.0333\n",
      "Coefficient plot saved to ./dep_delay_lr/year_2023\\plots\\logistic_regression_coefficients_2023.png\n",
      "\n",
      "Top 5 features increasing delay probability for 2023:\n",
      "                            Feature  Coefficient   AbsCoef\n",
      "25              cat__MKT_AIRLINE_WN     0.780318  0.780318\n",
      "16    cat__TIME_BLOCK_Night (18-21)     0.663971  0.663971\n",
      "34             cat__ORIGIN_IATA_DFW     0.634044  0.634044\n",
      "11  cat__TIME_BLOCK_Evening (15-18)     0.555629  0.555629\n",
      "26             cat__ORIGIN_IATA_ATL     0.483712  0.483712\n",
      "\n",
      "Top 5 features decreasing delay probability for 2023:\n",
      "                                Feature  Coefficient   AbsCoef\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)    -0.993167  0.993167\n",
      "15        cat__TIME_BLOCK_Morning (6-9)    -0.716189  0.716189\n",
      "47                 cat__ORIGIN_IATA_MSP    -0.492601  0.492601\n",
      "35                 cat__ORIGIN_IATA_DTW    -0.433561  0.433561\n",
      "51                 cat__ORIGIN_IATA_SAN    -0.380621  0.380621\n",
      "\n",
      "Day of week coefficients for 2023 (classification model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "91  cat__IS_WEEKEND_1    -0.015116  0.015116\n",
      "90  cat__IS_WEEKEND_0     0.013979  0.013979\n",
      "\n",
      "Top 5 features increasing delay duration for 2023:\n",
      "                            Feature  Coefficient    AbsCoef\n",
      "16    cat__TIME_BLOCK_Night (18-21)    10.154854  10.154854\n",
      "34             cat__ORIGIN_IATA_DFW     9.303333   9.303333\n",
      "21              cat__MKT_AIRLINE_F9     8.153740   8.153740\n",
      "22              cat__MKT_AIRLINE_G4     8.052347   8.052347\n",
      "11  cat__TIME_BLOCK_Evening (15-18)     6.898622   6.898622\n",
      "\n",
      "Top 5 features decreasing delay duration for 2023:\n",
      "                                Feature  Coefficient    AbsCoef\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)   -12.096794  12.096794\n",
      "18                  cat__MKT_AIRLINE_AS    -7.869554   7.869554\n",
      "35                 cat__ORIGIN_IATA_DTW    -5.634639   5.634639\n",
      "12     cat__TIME_BLOCK_Late Night (0-3)    -5.332231   5.332231\n",
      "15        cat__TIME_BLOCK_Morning (6-9)    -5.245517   5.245517\n",
      "\n",
      "Day of week coefficients for 2023 (regression model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "91  cat__IS_WEEKEND_1    -0.168965  0.168965\n",
      "90  cat__IS_WEEKEND_0     0.168965  0.168965\n",
      "Models saved to ./dep_delay_lr/year_2023\n",
      "\n",
      "Logistic Regression model training for 2023 complete! Total processing time: 93.04 seconds\n",
      "\n",
      "Model for year 2023 completed successfully!\n",
      "\n",
      "================================================================================\n",
      "Training model for year 2024\n",
      "================================================================================\n",
      "\n",
      "Processing May2024.csv...\n",
      "Years found in data: [2024]\n",
      "Months found in data: {5: 649428}\n",
      "Filtered to only May data: 649428 rows\n",
      "Filtered from 649428 to 228159 rows for top 30 airports\n",
      "Removed 2994.0 cancelled flights, remaining: 225165\n",
      "Processing took: 3.42 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/225165 rows, matched 7108 flights with weather data\n",
      "Processed 20000/225165 rows, matched 14410 flights with weather data\n",
      "Processed 30000/225165 rows, matched 21856 flights with weather data\n",
      "Processed 40000/225165 rows, matched 29104 flights with weather data\n",
      "Processed 50000/225165 rows, matched 36479 flights with weather data\n",
      "Processed 60000/225165 rows, matched 43708 flights with weather data\n",
      "Processed 70000/225165 rows, matched 50965 flights with weather data\n",
      "Processed 80000/225165 rows, matched 58539 flights with weather data\n",
      "Processed 90000/225165 rows, matched 65682 flights with weather data\n",
      "Processed 100000/225165 rows, matched 73009 flights with weather data\n",
      "Processed 110000/225165 rows, matched 80433 flights with weather data\n",
      "Processed 120000/225165 rows, matched 87679 flights with weather data\n",
      "Processed 130000/225165 rows, matched 95122 flights with weather data\n",
      "Processed 140000/225165 rows, matched 102419 flights with weather data\n",
      "Processed 150000/225165 rows, matched 109622 flights with weather data\n",
      "Processed 160000/225165 rows, matched 117118 flights with weather data\n",
      "Processed 170000/225165 rows, matched 124380 flights with weather data\n",
      "Processed 180000/225165 rows, matched 131691 flights with weather data\n",
      "Processed 190000/225165 rows, matched 139073 flights with weather data\n",
      "Processed 200000/225165 rows, matched 146378 flights with weather data\n",
      "Processed 210000/225165 rows, matched 153882 flights with weather data\n",
      "Processed 220000/225165 rows, matched 161154 flights with weather data\n",
      "Processed 225165/225165 rows, matched 165140 flights with weather data\n",
      "Matched weather data for 165140 flights (73.34%)\n",
      "Weather matching took: 78.68 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2024...\n",
      "Identified 6159 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 8151 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 13987 out of 225165 total flights (6.21%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 84574 flights (37.56%)\n",
      "  - Afternoon (12-18): 77071 flights (34.23%)\n",
      "  - Evening (18-24): 51803 flights (23.01%)\n",
      "  - Early Morning (0-6): 11717 flights (5.20%)\n",
      "\n",
      "Preparing delay data for 2024...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 105538/225165 (46.87%)\n",
      "On-time or early flights: 119627/225165 (53.13%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 20.52 minutes\n",
      "Median delay: 0.00 minutes\n",
      "Min delay: -31.00 minutes (negative means early departure)\n",
      "Max delay: 2232.00 minutes\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 451 flights (0.20%)\n",
      "  - Early: 119176 flights (52.93%)\n",
      "  - On Time: 43825 flights (19.46%)\n",
      "  - Moderate Delay: 36901 flights (16.39%)\n",
      "  - Significant Delay: 14300 flights (6.35%)\n",
      "  - Severe Delay: 10512 flights (4.67%)\n",
      "\n",
      "Creating time block features for 2024...\n",
      "\n",
      "Creating day features for 2024...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Friday: 37632 flights (16.71%)\n",
      "  - Thursday: 37381 flights (16.60%)\n",
      "  - Wednesday: 36037 flights (16.00%)\n",
      "  - Monday: 30205 flights (13.41%)\n",
      "  - Sunday: 28931 flights (12.85%)\n",
      "  - Tuesday: 28826 flights (12.80%)\n",
      "  - Saturday: 26153 flights (11.62%)\n",
      "\n",
      "Weekend flights: 55084 (24.46%)\n",
      "Weekday flights: 170081 (75.54%)\n",
      "\n",
      "Selecting features for delay prediction for 2024...\n",
      "Using categorical features: ['DAY_NAME', 'TIME_BLOCK', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK']\n",
      "Using numerical features: ['DISTANCE', 'PRCP']\n",
      "Training set size: (202648, 12)\n",
      "Test set size: (22517, 12)\n",
      "\n",
      "Training delay classification model for 2024 (Logistic Regression)...\n",
      "Classification model training took: 2.18 seconds\n",
      "\n",
      "Training delay regression model for 2024 (Linear Regression)...\n",
      "Regression model training took: 0.76 seconds\n",
      "\n",
      "Evaluating delay classification model for 2024...\n",
      "Classification Accuracy: 65.79%\n",
      "Classification ROC AUC: 0.7185\n",
      "Classification Precision (Delayed): 0.6224\n",
      "Classification Recall (Delayed): 0.6868\n",
      "Classification F1 Score (Delayed): 0.6530\n",
      "\n",
      "Evaluating delay regression model for 2024...\n",
      "Regression Mean Squared Error: 3531.07\n",
      "Regression Root Mean Squared Error: 59.42 minutes\n",
      "Regression Mean Absolute Error: 29.35 minutes\n",
      "Regression R² Score: 0.0556\n",
      "Coefficient plot saved to ./dep_delay_lr/year_2024\\plots\\logistic_regression_coefficients_2024.png\n",
      "\n",
      "Top 5 features increasing delay probability for 2024:\n",
      "                               Feature  Coefficient   AbsCoef\n",
      "34                cat__ORIGIN_IATA_DFW     0.867776  0.867776\n",
      "25                 cat__MKT_AIRLINE_WN     0.750535  0.750535\n",
      "83                  cat__DEST_IATA_SFO     0.710771  0.710771\n",
      "16       cat__TIME_BLOCK_Night (18-21)     0.675882  0.675882\n",
      "13  cat__TIME_BLOCK_Late Night (21-24)     0.588378  0.588378\n",
      "\n",
      "Top 5 features decreasing delay probability for 2024:\n",
      "                                Feature  Coefficient   AbsCoef\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)    -1.274979  1.274979\n",
      "15        cat__TIME_BLOCK_Morning (6-9)    -0.970631  0.970631\n",
      "47                 cat__ORIGIN_IATA_MSP    -0.460459  0.460459\n",
      "35                 cat__ORIGIN_IATA_DTW    -0.352404  0.352404\n",
      "24                  cat__MKT_AIRLINE_UA    -0.323246  0.323246\n",
      "\n",
      "Day of week coefficients for 2024 (classification model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "91  cat__IS_WEEKEND_1    -0.078040  0.078040\n",
      "90  cat__IS_WEEKEND_0     0.045237  0.045237\n",
      "\n",
      "Top 5 features increasing delay duration for 2024:\n",
      "                          Feature  Coefficient    AbsCoef\n",
      "22            cat__MKT_AIRLINE_G4    34.964834  34.964834\n",
      "34           cat__ORIGIN_IATA_DFW    20.046625  20.046625\n",
      "64             cat__DEST_IATA_DFW    11.775659  11.775659\n",
      "16  cat__TIME_BLOCK_Night (18-21)    11.376039  11.376039\n",
      "27           cat__ORIGIN_IATA_AUS     9.483827   9.483827\n",
      "\n",
      "Top 5 features decreasing delay duration for 2024:\n",
      "                                Feature  Coefficient    AbsCoef\n",
      "10  cat__TIME_BLOCK_Early Morning (3-6)   -16.211418  16.211418\n",
      "15        cat__TIME_BLOCK_Morning (6-9)   -12.683950  12.683950\n",
      "18                  cat__MKT_AIRLINE_AS   -10.018323  10.018323\n",
      "47                 cat__ORIGIN_IATA_MSP    -8.945950   8.945950\n",
      "20                  cat__MKT_AIRLINE_DL    -8.205904   8.205904\n",
      "\n",
      "Day of week coefficients for 2024 (regression model):\n",
      "              Feature  Coefficient   AbsCoef\n",
      "90  cat__IS_WEEKEND_0     2.436288  2.436288\n",
      "91  cat__IS_WEEKEND_1    -2.436288  2.436288\n",
      "Models saved to ./dep_delay_lr/year_2024\n",
      "\n",
      "Logistic Regression model training for 2024 complete! Total processing time: 88.27 seconds\n",
      "\n",
      "Model for year 2024 completed successfully!\n",
      "\n",
      "Comparing models across years...\n",
      "Comparison summary saved to ./dep_delay_lr/comparison\\year_model_comparison.csv\n",
      "Day of week coefficients by year saved to ./dep_delay_lr/comparison\\day_of_week_coefficients_by_year.json\n",
      "Feature coefficient comparison saved to ./dep_delay_lr/comparison\\feature_coefficient_comparison.json\n",
      "Year model comparison completed!\n",
      "\n",
      "Year-by-Year Model Training Summary:\n",
      "\n",
      "Year 2021:\n",
      "  Total flights: 171,382\n",
      "  Classification accuracy: 64.44%\n",
      "  Classification AUC: 0.6881\n",
      "  Regression RMSE: 38.30 minutes\n",
      "  Regression R²: 0.0364\n",
      "  Mean delay: 6.99 minutes\n",
      "  Delay rate: 32.01%\n",
      "\n",
      "  Day of week patterns for 2021:\n",
      "\n",
      "Year 2022:\n",
      "  Total flights: 205,420\n",
      "  Classification accuracy: 63.29%\n",
      "  Classification AUC: 0.6861\n",
      "  Regression RMSE: 49.36 minutes\n",
      "  Regression R²: 0.0331\n",
      "  Mean delay: 14.56 minutes\n",
      "  Delay rate: 44.43%\n",
      "\n",
      "  Day of week patterns for 2022:\n",
      "\n",
      "Year 2023:\n",
      "  Total flights: 219,176\n",
      "  Classification accuracy: 64.26%\n",
      "  Classification AUC: 0.7026\n",
      "  Regression RMSE: 49.57 minutes\n",
      "  Regression R²: 0.0333\n",
      "  Mean delay: 12.35 minutes\n",
      "  Delay rate: 40.07%\n",
      "\n",
      "  Day of week patterns for 2023:\n",
      "\n",
      "Year 2024:\n",
      "  Total flights: 225,165\n",
      "  Classification accuracy: 65.79%\n",
      "  Classification AUC: 0.7185\n",
      "  Regression RMSE: 59.42 minutes\n",
      "  Regression R²: 0.0556\n",
      "  Mean delay: 20.52 minutes\n",
      "  Delay rate: 46.87%\n",
      "\n",
      "  Day of week patterns for 2024:\n",
      "\n",
      "Training complete! Check output directories for detailed results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
